{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f971ed",
   "metadata": {},
   "source": [
    "# 3주차 강의정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4234b3a",
   "metadata": {},
   "source": [
    "<aside>\n",
    "✅ 이전 내용\n",
    "\n",
    "Multinomial classification\n",
    ": n개의 class로 분류를 할 때 n개의 독립된 clasifier들을 가지고 구현이 가능함.\n",
    "\n",
    "Hypothesis를 구하기 위한 벡터 연산 표현\n",
    "→\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/92504386/196467214-b14eeda6-963d-4b2a-b010-c11bff7244f5.png)\n",
    "\n",
    "\n",
    "</aside>\n",
    "\n",
    "## lec 6-1.\n",
    "\n",
    "Logistic classifier를 벡터로 표현해서 계산하게 되면, 입력값에 대해 A로 분류될 값, B로 분류될 값, C로 분류될 값이 나오게 됨. \n",
    "\n",
    "이때 예측값을 0과 1사이의 값의 확률로 표현하고 이들의 합이 1이 되게끔 표현하고자 함.\n",
    "\n",
    "이때 필요한 것이 Softmax 함수임.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/92504386/196467825-38b629a6-780c-48b3-8130-c05ebdb3dbb9.png)\n",
    "\n",
    "## Softmax Regression\n",
    "\n",
    "### Softmax 함수\n",
    "\n",
    ": 소프트맥스는 **세 개 이상으로 분류하는 다중 클래스 분류에서 사용되는 활성화 함수**다. 소프트맥스 함수는 분류될 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정한다\n",
    "\n",
    "### One - Hot Encoding\n",
    "\n",
    ": softmax함수를 통해 나온 예측값 중 가장 큰 값을 1.0으로 만들고, 나머지는 0으로 하는 기법\n",
    "\n",
    "→ 텐서플로우의 ‘argmax’ 함수를 통해 encoding 가능\n",
    "\n",
    "예측 값과 실제값의 차이 Cosf function의 값을 작게 훈련시킴으로써 모델 학습.\n",
    "\n",
    "### Cross-Entropy\n",
    "\n",
    "softmax함수를 통해서 얻어낸 예측값 s(y)=Y’ 실제 값 Y   \n",
    "\n",
    "두 값 사이의 차이를 cross-entropy 함수를 통해 구함.\n",
    "\n",
    "$D(Y’, Y) = - ∑ Yi * log(Y’i)$ \n",
    "\n",
    "→ Y’i : softmax를 통해 얻은 값이므로 항상 0~1 사이의 값만을 가짐.\n",
    "\n",
    "→  $-log(Y’i)$ (← logistic regression) 0일 때는 무한대에 가깝게 가고, y의 값이 1에 가까워질 수록 0에 가까워짐.\n",
    "\n",
    "- cost function의 궁극적인 목표 : 예측이 맞을 때는 값을 작게, 예측이 틀릴 때는 값을 크게 해줌.\n",
    "    \n",
    "     틀리면 ∞, 맞으면 0\n",
    "    \n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/92504386/196468064-7811b5c7-796a-4a1a-ba48-be1afeb92e32.png)\n",
    "\n",
    "\n",
    "\n",
    "Loss function을 이용해서\n",
    "\n",
    "여러개의 training set이 있을 때는 전체의 차이를 구한 후 평균을 내줌.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/92504386/196468255-8ec3b292-3710-4cdc-83c4-0d64fbe23e84.png)\n",
    "\n",
    "마지막 단계)\n",
    "\n",
    "Gradient descent\n",
    "\n",
    "cost 를 최소화 하는 w,b를 찾는 Gradient descent 기법 이용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1fd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadc032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c60ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tf.executing_eagerly()\n",
    "#tf.set_random_seed(777)  # for reproducibility\n",
    "tf.random.set_seed(777)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e81aac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "#convert into numpy and float format\n",
    "x_data = np.asarray(x_data, dtype=np.float32)\n",
    "y_data = np.asarray(y_data, dtype=np.float32)\n",
    "\n",
    "nb_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4810a71",
   "metadata": {},
   "source": [
    "cross entropy = Y * log(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf688350",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.normal([4, nb_classes]), name='wegiht')\n",
    "b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
    "\n",
    "variables = [W,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd0cf06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#cross entropy cost / loss\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# cost함수 최소화\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m cost \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\u001b[38;5;241m-\u001b[39mtf\u001b[38;5;241m.\u001b[39mreduce_sum(\u001b[43mY\u001b[49m \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mlog(hypothesis), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mGradientDescentOptimizer(learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m.\u001b[39mminimize(cost))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "#cross entropy cost / loss\n",
    "\n",
    "# cost함수 최소화\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1.minimize(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0159c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.36571964e-02 7.90162385e-03 9.78441238e-01]\n",
      " [3.92597765e-02 1.70347411e-02 9.43705440e-01]\n",
      " [3.80385250e-01 1.67723164e-01 4.51891571e-01]\n",
      " [3.23390484e-01 5.90759404e-02 6.17533624e-01]\n",
      " [3.62997389e-06 6.20727292e-08 9.99996305e-01]\n",
      " [2.62520202e-02 1.07279625e-02 9.63019967e-01]\n",
      " [1.56525111e-05 4.21802753e-07 9.99983907e-01]\n",
      " [2.94076904e-06 3.81133276e-08 9.99997020e-01]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "print(hypothesis(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affad630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1656e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.9302204  0.06200533 0.00777428]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Softmax onehot test\n",
    "sample_db = [[8,2,1,4]]\n",
    "sample_db = np.asarray(sample_db, dtype=np.float32)\n",
    "\n",
    "print(hypothesis(sample_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f486afb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.07932, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cost_fn(X, Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1)\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    \n",
    "    return cost_mean\n",
    "\n",
    "print(cost_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b725e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[ 0.06914607, -0.6509784 ,  0.5818323 ],\n",
      "       [-1.5221257 , -1.214863  ,  2.7369885 ],\n",
      "       [-1.2473828 , -1.7611003 ,  3.008483  ],\n",
      "       [-1.2014606 , -1.8659233 ,  3.0673838 ]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.15212913, -0.34219202,  0.4943211 ], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "\n",
    "        return grads\n",
    "\n",
    "print(grad_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "465d5f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 5.682034\n",
      "Loss at epoch 100: 1.067572\n",
      "Loss at epoch 200: 0.741777\n",
      "Loss at epoch 300: 0.662876\n",
      "Loss at epoch 400: 0.630050\n",
      "Loss at epoch 500: 0.606943\n",
      "Loss at epoch 600: 0.587981\n",
      "Loss at epoch 700: 0.571635\n",
      "Loss at epoch 800: 0.557224\n",
      "Loss at epoch 900: 0.544342\n",
      "Loss at epoch 1000: 0.532706\n",
      "Loss at epoch 1100: 0.522104\n",
      "Loss at epoch 1200: 0.512375\n",
      "Loss at epoch 1300: 0.503388\n",
      "Loss at epoch 1400: 0.495040\n",
      "Loss at epoch 1500: 0.487248\n",
      "Loss at epoch 1600: 0.479941\n",
      "Loss at epoch 1700: 0.473063\n",
      "Loss at epoch 1800: 0.466564\n",
      "Loss at epoch 1900: 0.460404\n",
      "Loss at epoch 2000: 0.454548\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "def fit(X, Y, epochs=2000, verbose=100):\n",
    "    #optimizer =  tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    \n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            \n",
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59ef08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea46063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4da46a98",
   "metadata": {},
   "source": [
    "# Lab 6-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69f1f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#tf.enable_eager_execution()\n",
    "tf.random.set_seed(777)\n",
    "\n",
    "#tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b528871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 1 0]\n",
      " [1 0 0 ... 0 1 0]\n",
      " [0 0 1 ... 0 0 3]\n",
      " ...\n",
      " [1 0 0 ... 0 1 0]\n",
      " [0 0 1 ... 0 0 6]\n",
      " [0 1 1 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.int32) #tf1.13.1에서는 np.int32, 이전에는 np.float32\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7de70f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "# Make Y data as onehot shape\n",
    "Y_one_hot = tf.one_hot(list(y_data), nb_classes)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "\n",
    "print(x_data.shape, Y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0717b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight and bias setting\n",
    "W = tf.Variable(tf.random.normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
    "variables = [W, b]\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def logit_fn(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(logit_fn(X))\n",
    "\n",
    "def cost_fn(X, Y):\n",
    "    logits = logit_fn(X)\n",
    "    cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                           labels=Y)\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        return grads\n",
    "    \n",
    "def prediction(X, Y):\n",
    "    pred = tf.argmax(hypothesis(X), 1)\n",
    "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77ecc3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 1]\n",
      " [1 0 0 ... 1 0 1]\n",
      " [0 0 1 ... 1 0 0]\n",
      " ...\n",
      " [1 0 0 ... 1 0 1]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 1 ... 1 0 0]] [[0]\n",
      " [0]\n",
      " [3]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [3]\n",
      " [6]\n",
      " [6]\n",
      " [6]\n",
      " [1]\n",
      " [0]\n",
      " [3]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [5]\n",
      " [4]\n",
      " [4]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [5]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [3]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [3]\n",
      " [5]\n",
      " [5]\n",
      " [1]\n",
      " [5]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [6]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [5]\n",
      " [4]\n",
      " [6]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [3]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [6]\n",
      " [3]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [6]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [6]\n",
      " [3]\n",
      " [1]\n",
      " [0]\n",
      " [6]\n",
      " [3]\n",
      " [1]\n",
      " [5]\n",
      " [4]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [5]\n",
      " [0]\n",
      " [6]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(x_data, y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e5f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d59e063",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute MatMul as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m|\u001b[39m ((i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m%\u001b[39mverbose\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss at epoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, cost_fn(X, Y)\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[1;32m---> 12\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(X, Y, epochs, verbose)\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mGradientDescentOptimizer(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 7\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, variables))\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m|\u001b[39m ((i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m%\u001b[39mverbose\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mgrad_fn\u001b[1;34m(X, Y)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_fn\u001b[39m(X, Y):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 24\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcost_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m         grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, variables)\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m grads\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mcost_fn\u001b[1;34m(X, Y)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost_fn\u001b[39m(X, Y):\n\u001b[1;32m---> 15\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     cost_i \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax_cross_entropy_with_logits_v2(logits\u001b[38;5;241m=\u001b[39mlogits,\n\u001b[0;32m     17\u001b[0m                                                            labels\u001b[38;5;241m=\u001b[39mY)\n\u001b[0;32m     18\u001b[0m     cost \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(cost_i)\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mlogit_fn\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogit_fn\u001b[39m(X):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: cannot compute MatMul as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=2000, verbose=500):\n",
    "    #optimizer =  tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    \n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            \n",
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcf71b32",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'Matmul'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mx_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMatmul\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'Matmul'"
     ]
    }
   ],
   "source": [
    "x_data.Matmul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25cfd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
