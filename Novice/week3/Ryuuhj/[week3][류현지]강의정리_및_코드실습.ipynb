{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df25d5c",
   "metadata": {},
   "source": [
    "# **Lec 5. Softmax classification**\n",
    "\n",
    "## **📖 Multinomial classification : Softmax classification**\n",
    "\n",
    " **배경) logistic regression**\n",
    " - 기존에 연속적인 numeric한 값으로 표현되는 함수 → sigmoid 함수를 통해 binary 한 값으로 조정\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5efb277c-bb3a-447a-89f5-c0a901c46f0d/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T151016Z&X-Amz-Expires=86400&X-Amz-Signature=22a188942837026d8e259b8ae5ad4c75c413ac3191bf1053f1552356f35726b9&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\n",
    "**Multinomial classification**\n",
    "- multinomial : 여러 개의 class가 존재\n",
    "- binary classification만으로도 multinomial classificaiton 구현이 가능함\n",
    "\t- EX) A, B, C 3가지의 class로 분류할 경우 \n",
    "\t\t1. Binary 1 : C와 C가 아닌것으로 분류 (C or Not)\n",
    "\t\t2. Binary 2 : B와 B가 아닌 것 (B or Not)\n",
    "\t\t3. Binary 3 : A와 A가 아닌 것 (A or Not)\n",
    "\t\t∴ 3개의 binary classification로 구현 가능\n",
    "\t\t![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/cad83e16-c494-4a06-8c9a-a8ab4641a8ce/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T152107Z&X-Amz-Expires=86400&X-Amz-Signature=59340a87b504f582b65128cd5104a2ed9e77b22b9b459de3f33748bca14dd7e8&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t\t- A, B, C 관련해 각각 독립된 형태의 벡터로 계산해 도출\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/bb0903e9-8eda-4b43-9fa4-8184379cbfc8/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T152719Z&X-Amz-Expires=86400&X-Amz-Signature=5e2748d91bda405d8906a0c41793209e4ac9f16066b1393fa59898ab29f69a02&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t\t- But 복잡하기에 하나로 합쳐서 구현 → 원하는 값 A, B, C 관련 가설 값 구함\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e31aa729-0af3-4d87-9553-f5f41934ab5a/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T152857Z&X-Amz-Expires=86400&X-Amz-Signature=75fb6269372b84abc9835acdf0a2d6427738d63be6afaa78daabc1d3a92adc9c&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t\t\t> H_A(x) ∼ Y_A\n",
    "\t\n",
    "\t\t👉 하나의 vector로 처리하면 한번에 계산 가능하면서 독립된 classification처럼 동작함 \n",
    "\t\t(즉 하나의 classifier을 여러 개의 독립된 classifier처럼 구현 가능함)\n",
    "\n",
    "\t❓ 그렇다면 중간에 sigmoid 처리는 각각 해줘야 하나? → 얘도 한번에 가능!\n",
    "\t\t\n",
    "\n",
    "## 📖 softmax classifier 의 cost function\n",
    "\n",
    "**결과 벡터 안 Y값들의 범위 축소**\n",
    "- Logistic classifier에 data X를 줘서 계산하면 각 classifier을 거친 값 Y_A, B, C가 벡터 형태로 도출됨\n",
    "\t- 이때 나오는 값들은 범위 제한 없이 다양\n",
    "\t- 따라서 0~1 범위로 정규화가 필요함\n",
    "\t\t- ∑(classifier들의 정규화 된 Y값) = 1\n",
    "\t ✅이렇게 조정해 주는 역할 ⟹ Softmax\n",
    "\t \n",
    "📌**Softmax**\n",
    "- n개의 값을 Softmax에 넣으면 range(0,1) 값들로 변환해 줌 (score → Probabilities)\n",
    "- **특징**\n",
    "\t1. 결과 값이 0 ~ 1 범위 내 존재\n",
    "\t2. ∑(softmax 결과 값) = 1 \n",
    "\t👉 각각의 결과 값을 확률로 볼 수 있음 (ex. A, B, C가 나올 확률)\n",
    "- One-Hot Encoding 기법\n",
    "\t- 이 확률들 중 제일 가능성이 높은 한 가지만 알려주도록 처리\n",
    "\t- P값들 중 제일 크기 큰 값 = 1, 나머지 0 처리\n",
    "\t- 이렇게 나온 최종 결과를 통해 가장 가까운 후보를 최종 결정\n",
    "\n",
    "\t> **예측 모델 Hypothesis가 완성됨** \n",
    "\t→이렇게 예측한 값과 실제 값의 차이가 어느정도인지 나타내는 Cost function 정의 필요 (→ Cost function을 최소화 함으로써 학습 완성)\n",
    "\n",
    "📌**Cost function**\n",
    "- Cross-Entropy 사용\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/320c32da-ae96-40c1-b87f-e320278a51ee/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T164213Z&X-Amz-Expires=86400&X-Amz-Signature=97d372b470e47898c13901efad16bfa13ff3a221702e25bd4fb21ecc9fc94fde&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t - L =실제 값, S(Y) = Softmax를 거쳐 도출된 예측 값\n",
    "\t - 이 2개의 차이 D(S, L)를 Cross Entropy를 통해 구함\n",
    "\t \n",
    "\t**📌 Cross-entropy 함수가 cost function에 사용되는 이유**\n",
    "\t- -log(y_i)은 항상 0 ~ 1사이 값을 가짐\n",
    "\t- EX) 2가지 Label이 있다고 가정, L(실제 값=정답) ⟹ B를 예측함\n",
    "\t![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/cb6512d3-e1c1-44bd-a221-bebb4ebd0939/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T165010Z&X-Amz-Expires=86400&X-Amz-Signature=e2d6dc16737045a513ab33e2ec587f7bbadab002871024aa406e46d224d27bf0&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t- 예측 가능한 경우 1 : Y의 예측이 [0, 1]로 나온 경우\n",
    "\t\t- Y가  B를 예측⟹ 맞은 예측\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/14c9bc8f-5e24-41e5-9018-692f57ecce47/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T165750Z&X-Amz-Expires=86400&X-Amz-Signature=935b89c837db8c9b69f8d6fe9acfe3e8c0f1a5e1afffbb114a9c253c79e271d0&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t\t- cost 함수의 최종 값 0 --- good! \n",
    "\t\t\n",
    "\t- 예측 가능한 경우 2: Y의 예측이 [1, 0]로 나온 경우\n",
    "\t\t- Y가 A를 예\n",
    "\t\t- 측 ⟹ 틀린 예측, 값을 엄청 크게 만들어서 티냄\n",
    "\t![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/c99ce0ce-c266-4049-b031-2209cd2655aa/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T170036Z&X-Amz-Expires=86400&X-Amz-Signature=b273c868939dd2a1e5163ca040a6ec5128077fb5885038699110f20727e17fc6&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t\t- 따라서 예측 값이 틀린 경우 cost값이 무한대로 증가 --- 예측이 틀림\n",
    "\t\t\n",
    "**📌 1개의 데이터가 아닌 n개의 dataset이 주어지는 경우 cost function**\n",
    "- 전체의 거리(= D )를 구한 뒤 n으로 나눠 평균 구함\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/1a4ea154-0799-4209-b458-e4d35fadd2b8/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T171206Z&X-Amz-Expires=86400&X-Amz-Signature=39a337a9fff683db812318617b56d5e694885f454c45e66422c8b97b4c67b8a8&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "- 이렇게 **전체 Loss(= Cost function )** 정의 가능 \n",
    "\n",
    "## 📖 softmax classifier 의 Gradient descent\n",
    "- 목적\n",
    "\t- cost를 최소화 하는 W 벡터 찾기 위해 Gradient descent사용\n",
    "\t![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/44512fb5-0efc-460a-b804-90fcbb84e487/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T171620Z&X-Amz-Expires=86400&X-Amz-Signature=52bfdfa976aee69b9e1ae8027bdecb45a0ebed485f81b97a18d2749552e07b88&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t- 양상은 logistic function과 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f3549",
   "metadata": {},
   "source": [
    "# LAB 6-1 : Softmax Classifier 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "968550b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.random.set_seed(777) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d2b0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data = (8*4) -> 4개의 feature\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "# y_data =(8*3) \n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "#convert into numpy and float : 학습에 용이하게 하기 위해 numpy 형태로 변환\n",
    "x_data = np.asarray(x_data, dtype=np.float32)\n",
    "y_data = np.asarray(y_data, dtype=np.float32) #one-hot 표현 시 y의 개수= label(class)의 개수\n",
    "\n",
    "nb_classes = 3 #class의 개수\n",
    "#one-hot encoding 방법 - 제일 큰 값만 1, 나머지는 0으로 표기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c53d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight and bias setting     #4:입력되는 값 (x의 값 = 4개) 3: 나가는 값 (y의 값 - class 값)\n",
    "W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias') #출력값 - class 개수\n",
    "variables = [W, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33181d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.36571955e-02 7.90162291e-03 9.78441179e-01]\n",
      " [3.92597765e-02 1.70347411e-02 9.43705440e-01]\n",
      " [3.80385250e-01 1.67723149e-01 4.51891541e-01]\n",
      " [3.23390484e-01 5.90759404e-02 6.17533624e-01]\n",
      " [3.62997366e-06 6.20727221e-08 9.99996245e-01]\n",
      " [2.62520202e-02 1.07279625e-02 9.63019967e-01]\n",
      " [1.56525093e-05 4.21802724e-07 9.99983847e-01]\n",
      " [2.94076904e-06 3.81133241e-08 9.99996960e-01]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b) #softmax를 이용해 hypothesis 완성하기\n",
    "\n",
    "print(hypothesis(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d2ace6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.9302204  0.06200533 0.00777428]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Softmax onehot test\n",
    "sample_db = [[8,2,1,4]]\n",
    "sample_db = np.asarray(sample_db, dtype=np.float32)\n",
    "\n",
    "print(hypothesis(sample_db)) #onehot encoding 작동 테스트 (1개만 1 나머진 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c755d326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.07932, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cost_fn(X, Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1) #가설을 이용해 가설 - 실제값으로 cost함수\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    \n",
    "    return cost_mean\n",
    "\n",
    "print(cost_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f45f9da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#이건 그냥 기울기 구하는 with GradientTape() 테스트 - (3,0)에서의 기울기\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x # x^2\n",
    "dy_dx = g.gradient(y, x) # Will compute to 6.0\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43b1c382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[ 0.06914607, -0.6509784 ,  0.5818323 ],\n",
      "       [-1.5221257 , -1.214863  ,  2.7369885 ],\n",
      "       [-1.2473829 , -1.7611003 ,  3.0084827 ],\n",
      "       [-1.2014607 , -1.8659232 ,  3.0673835 ]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.15212914, -0.342192  ,  0.49432108], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "#Gradient Function 구성\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y) #loss = 사전에 정의한 cost function \n",
    "        grads = tape.gradient(loss, variables) #grads = gradient(loss, (W, b)값)로 grad 구하기\n",
    "\n",
    "        return grads\n",
    "\n",
    "print(grad_fn(x_data, y_data)) #경사 변화량확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c16051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 0.159427\n",
      "Loss at epoch 100: 0.154003\n",
      "Loss at epoch 200: 0.148872\n",
      "Loss at epoch 300: 0.144060\n",
      "Loss at epoch 400: 0.139539\n",
      "Loss at epoch 500: 0.135282\n",
      "Loss at epoch 600: 0.131269\n",
      "Loss at epoch 700: 0.127480\n",
      "Loss at epoch 800: 0.123896\n",
      "Loss at epoch 900: 0.120502\n",
      "Loss at epoch 1000: 0.117282\n",
      "Loss at epoch 1100: 0.114226\n",
      "Loss at epoch 1200: 0.111319\n",
      "Loss at epoch 1300: 0.108553\n",
      "Loss at epoch 1400: 0.105917\n",
      "Loss at epoch 1500: 0.103402\n",
      "Loss at epoch 1600: 0.101001\n",
      "Loss at epoch 1700: 0.098706\n",
      "Loss at epoch 1800: 0.096510\n",
      "Loss at epoch 1900: 0.094407\n",
      "Loss at epoch 2000: 0.092391\n"
     ]
    }
   ],
   "source": [
    "# 위에서 구현한 함수 기반으로 학습모델 생성하는 fit 함수 정의\n",
    "\n",
    "def fit(X, Y, epochs=2000, verbose=100):\n",
    "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1) #optimizer 정의 (learning rate = 0.1)\n",
    "    #gradient 사용해 값 업데이트 해줌\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0): #100번마다 loss값 출력하며 진행방향 올바른지 체크\n",
    "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            \n",
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f374c8f",
   "metadata": {},
   "source": [
    "## Prediction\n",
    ": 학습 후 Hypothesis를 통해 직접 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b59c33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시용 샘플 데이터\n",
    "sample_data = [[2,1,3,2]] # answer_label [[0,0,1]]\n",
    "sample_data = np.asarray(sample_data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d0f3074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4.3665306e-04 4.6358623e-02 9.5320475e-01]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 각각의 예측값에서 가장 높은 값을 뽑기 예시  - one hot encoding\n",
    "# hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "a = hypothesis(sample_data)\n",
    "\n",
    "print(a)\n",
    "#나온 확률 값 중 가장 높은 값 선택 -> argmax\n",
    "print(tf.argmax(a, 1)) #index: 2 -> [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9613aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.9436805e-08 1.5021235e-04 9.9984980e-01]\n",
      " [4.3665289e-04 4.6358626e-02 9.5320481e-01]\n",
      " [9.5460018e-10 9.6164070e-02 9.0383589e-01]\n",
      " [2.2774371e-07 9.1306245e-01 8.6937323e-02]\n",
      " [1.6050829e-01 8.3273548e-01 6.7562237e-03]\n",
      " [8.4970415e-02 9.1502690e-01 2.6765201e-06]\n",
      " [8.2284725e-01 1.7714940e-01 3.2996732e-06]\n",
      " [9.6834594e-01 3.1654030e-02 1.4558012e-08]], shape=(8, 3), dtype=float32)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#x_data로 적용\n",
    "\n",
    "b = hypothesis(x_data)\n",
    "print(b)\n",
    "print(tf.argmax(b, 1))\n",
    "print(tf.argmax(y_data, 1)) # Y와 일치함 - 학습 정상적으로 완료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852ecfd",
   "metadata": {},
   "source": [
    "# LAB 6-2 : Softmax Classifier Animal Classification\n",
    ": softmax classifier을 통해 동물 분류 해보기\n",
    "    - softmax_cross_entropy_with_logits 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e398319b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 7)\n"
     ]
    }
   ],
   "source": [
    "#Sample dataset 준비\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1] #마지막 column 제외한 만큼\n",
    "y_data = xy[:, -1]\n",
    "\n",
    "nb_classes = 7  # 클래스 숫자 정의 : 0 ~ 6 , 총 7종으로 분류 -> 종 예측\n",
    "\n",
    "#데이터 전처리\n",
    "# Y값 one-hot 형태로 변형 - tf.one_hot - 이 과정에서 차원 +1\n",
    "Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes) #astype으로 reshape\n",
    "\n",
    "print(x_data.shape, Y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac940de7",
   "metadata": {},
   "source": [
    "- softmax classifier 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e122e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Weight and bias 세팅\n",
    "W = tf.Variable(tf.random.normal((16, nb_classes)), name='weight')\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
    "variables = [W, b] #weight와 bias 업데이트할 variable 별도로 지정\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def logit_fn(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(logit_fn(X))\n",
    "#logit 사용\n",
    "def cost_fn(X, Y):\n",
    "    logits = logit_fn(X)\n",
    "    cost_i = tf.keras.losses.categorical_crossentropy(y_true=Y, y_pred=logits, \n",
    "                                                      from_logits=True)    \n",
    "    cost = tf.reduce_mean(cost_i)    \n",
    "    return cost\n",
    "#gradient 구함 \n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables) #W,b 사용\n",
    "        return grads\n",
    "#단순 loss 뿐만 아니라 정확도도 계산 (hypothesis 사용됨)\n",
    "def prediction(X, Y):\n",
    "    pred = tf.argmax(hypothesis(X), 1)\n",
    "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb766eb",
   "metadata": {},
   "source": [
    "### Implementation - training\n",
    ": 구현한 softmax 학습하는 함수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92792e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1 Loss: 3.635028839111328, Acc: 0.1683168262243271\n",
      "Steps: 100 Loss: 0.5194157958030701, Acc: 0.7920792102813721\n",
      "Steps: 200 Loss: 0.31850090622901917, Acc: 0.9108911156654358\n",
      "Steps: 300 Loss: 0.23534879088401794, Acc: 0.9405940771102905\n",
      "Steps: 400 Loss: 0.18872138857841492, Acc: 0.9504950642585754\n",
      "Steps: 500 Loss: 0.15846037864685059, Acc: 0.9504950642585754\n",
      "Steps: 600 Loss: 0.13703757524490356, Acc: 0.9900990128517151\n",
      "Steps: 700 Loss: 0.12098980695009232, Acc: 0.9900990128517151\n",
      "Steps: 800 Loss: 0.10847964882850647, Acc: 1.0\n",
      "Steps: 900 Loss: 0.09843041002750397, Acc: 1.0\n",
      "Steps: 1000 Loss: 0.09016557037830353, Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=1000, verbose=100):\n",
    "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            acc = prediction(X, Y).numpy() #prediction 함수를 통해 정확도까지 출력\n",
    "            loss = cost_fn(X, Y).numpy() \n",
    "            print('Steps: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))\n",
    "\n",
    "fit(x_data, Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d412a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
