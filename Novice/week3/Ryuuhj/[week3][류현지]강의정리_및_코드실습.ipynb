{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df25d5c",
   "metadata": {},
   "source": [
    "# **Lec 5. Softmax classification**\n",
    "\n",
    "## **ğŸ“– Multinomial classification : Softmax classification**\n",
    "\n",
    " **ë°°ê²½) logistic regression**\n",
    " - ê¸°ì¡´ì— ì—°ì†ì ì¸ numericí•œ ê°’ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” í•¨ìˆ˜ â†’ sigmoid í•¨ìˆ˜ë¥¼ í†µí•´ binary í•œ ê°’ìœ¼ë¡œ ì¡°ì •\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5efb277c-bb3a-447a-89f5-c0a901c46f0d/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T151016Z&X-Amz-Expires=86400&X-Amz-Signature=22a188942837026d8e259b8ae5ad4c75c413ac3191bf1053f1552356f35726b9&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\n",
    "**Multinomial classification**\n",
    "- multinomial : ì—¬ëŸ¬ ê°œì˜ classê°€ ì¡´ì¬\n",
    "- binary classificationë§Œìœ¼ë¡œë„ multinomial classificaiton êµ¬í˜„ì´ ê°€ëŠ¥í•¨\n",
    "\t- EX) A, B, C 3ê°€ì§€ì˜ classë¡œ ë¶„ë¥˜í•  ê²½ìš° \n",
    "\t\t1. Binary 1 : Cì™€ Cê°€ ì•„ë‹Œê²ƒìœ¼ë¡œ ë¶„ë¥˜ (C or Not)\n",
    "\t\t2. Binary 2 : Bì™€ Bê°€ ì•„ë‹Œ ê²ƒ (B or Not)\n",
    "\t\t3. Binary 3 : Aì™€ Aê°€ ì•„ë‹Œ ê²ƒ (A or Not)\n",
    "\t\tâˆ´ 3ê°œì˜ binary classificationë¡œ êµ¬í˜„ ê°€ëŠ¥\n",
    "\t\t![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/cad83e16-c494-4a06-8c9a-a8ab4641a8ce/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T152107Z&X-Amz-Expires=86400&X-Amz-Signature=59340a87b504f582b65128cd5104a2ed9e77b22b9b459de3f33748bca14dd7e8&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t\t- A, B, C ê´€ë ¨í•´ ê°ê° ë…ë¦½ëœ í˜•íƒœì˜ ë²¡í„°ë¡œ ê³„ì‚°í•´ ë„ì¶œ\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/bb0903e9-8eda-4b43-9fa4-8184379cbfc8/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T152719Z&X-Amz-Expires=86400&X-Amz-Signature=5e2748d91bda405d8906a0c41793209e4ac9f16066b1393fa59898ab29f69a02&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t\t- But ë³µì¡í•˜ê¸°ì— í•˜ë‚˜ë¡œ í•©ì³ì„œ êµ¬í˜„ â†’ ì›í•˜ëŠ” ê°’ A, B, C ê´€ë ¨ ê°€ì„¤ ê°’ êµ¬í•¨\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e31aa729-0af3-4d87-9553-f5f41934ab5a/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T152857Z&X-Amz-Expires=86400&X-Amz-Signature=75fb6269372b84abc9835acdf0a2d6427738d63be6afaa78daabc1d3a92adc9c&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t\t\t> H_A(x) âˆ¼ Y_A\n",
    "\t\n",
    "\t\tğŸ‘‰ í•˜ë‚˜ì˜ vectorë¡œ ì²˜ë¦¬í•˜ë©´ í•œë²ˆì— ê³„ì‚° ê°€ëŠ¥í•˜ë©´ì„œ ë…ë¦½ëœ classificationì²˜ëŸ¼ ë™ì‘í•¨ \n",
    "\t\t(ì¦‰ í•˜ë‚˜ì˜ classifierì„ ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ëœ classifierì²˜ëŸ¼ êµ¬í˜„ ê°€ëŠ¥í•¨)\n",
    "\n",
    "\tâ“ ê·¸ë ‡ë‹¤ë©´ ì¤‘ê°„ì— sigmoid ì²˜ë¦¬ëŠ” ê°ê° í•´ì¤˜ì•¼ í•˜ë‚˜? â†’ ì–˜ë„ í•œë²ˆì— ê°€ëŠ¥!\n",
    "\t\t\n",
    "\n",
    "## ğŸ“– softmax classifier ì˜ cost function\n",
    "\n",
    "**ê²°ê³¼ ë²¡í„° ì•ˆ Yê°’ë“¤ì˜ ë²”ìœ„ ì¶•ì†Œ**\n",
    "- Logistic classifierì— data Xë¥¼ ì¤˜ì„œ ê³„ì‚°í•˜ë©´ ê° classifierì„ ê±°ì¹œ ê°’ Y_A, B, Cê°€ ë²¡í„° í˜•íƒœë¡œ ë„ì¶œë¨\n",
    "\t- ì´ë•Œ ë‚˜ì˜¤ëŠ” ê°’ë“¤ì€ ë²”ìœ„ ì œí•œ ì—†ì´ ë‹¤ì–‘\n",
    "\t- ë”°ë¼ì„œ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”ê°€ í•„ìš”í•¨\n",
    "\t\t- âˆ‘(classifierë“¤ì˜ ì •ê·œí™” ëœ Yê°’) = 1\n",
    "\t âœ…ì´ë ‡ê²Œ ì¡°ì •í•´ ì£¼ëŠ” ì—­í•  âŸ¹ Softmax\n",
    "\t \n",
    "ğŸ“Œ**Softmax**\n",
    "- nê°œì˜ ê°’ì„ Softmaxì— ë„£ìœ¼ë©´ range(0,1) ê°’ë“¤ë¡œ ë³€í™˜í•´ ì¤Œ (score â†’ Probabilities)\n",
    "- **íŠ¹ì§•**\n",
    "\t1. ê²°ê³¼ ê°’ì´ 0 ~ 1 ë²”ìœ„ ë‚´ ì¡´ì¬\n",
    "\t2. âˆ‘(softmax ê²°ê³¼ ê°’) = 1 \n",
    "\tğŸ‘‰ ê°ê°ì˜ ê²°ê³¼ ê°’ì„ í™•ë¥ ë¡œ ë³¼ ìˆ˜ ìˆìŒ (ex. A, B, Cê°€ ë‚˜ì˜¬ í™•ë¥ )\n",
    "- One-Hot Encoding ê¸°ë²•\n",
    "\t- ì´ í™•ë¥ ë“¤ ì¤‘ ì œì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ í•œ ê°€ì§€ë§Œ ì•Œë ¤ì£¼ë„ë¡ ì²˜ë¦¬\n",
    "\t- Pê°’ë“¤ ì¤‘ ì œì¼ í¬ê¸° í° ê°’ = 1, ë‚˜ë¨¸ì§€ 0 ì²˜ë¦¬\n",
    "\t- ì´ë ‡ê²Œ ë‚˜ì˜¨ ìµœì¢… ê²°ê³¼ë¥¼ í†µí•´ ê°€ì¥ ê°€ê¹Œìš´ í›„ë³´ë¥¼ ìµœì¢… ê²°ì •\n",
    "\n",
    "\t> **ì˜ˆì¸¡ ëª¨ë¸ Hypothesisê°€ ì™„ì„±ë¨** \n",
    "\tâ†’ì´ë ‡ê²Œ ì˜ˆì¸¡í•œ ê°’ê³¼ ì‹¤ì œ ê°’ì˜ ì°¨ì´ê°€ ì–´ëŠì •ë„ì¸ì§€ ë‚˜íƒ€ë‚´ëŠ” Cost function ì •ì˜ í•„ìš” (â†’ Cost functionì„ ìµœì†Œí™” í•¨ìœ¼ë¡œì¨ í•™ìŠµ ì™„ì„±)\n",
    "\n",
    "ğŸ“Œ**Cost function**\n",
    "- Cross-Entropy ì‚¬ìš©\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/320c32da-ae96-40c1-b87f-e320278a51ee/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T164213Z&X-Amz-Expires=86400&X-Amz-Signature=97d372b470e47898c13901efad16bfa13ff3a221702e25bd4fb21ecc9fc94fde&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t - L =ì‹¤ì œ ê°’, S(Y) = Softmaxë¥¼ ê±°ì³ ë„ì¶œëœ ì˜ˆì¸¡ ê°’\n",
    "\t - ì´ 2ê°œì˜ ì°¨ì´ D(S, L)ë¥¼ Cross Entropyë¥¼ í†µí•´ êµ¬í•¨\n",
    "\t \n",
    "\t**ğŸ“Œ Cross-entropy í•¨ìˆ˜ê°€ cost functionì— ì‚¬ìš©ë˜ëŠ” ì´ìœ **\n",
    "\t- -log(y_i)ì€ í•­ìƒ 0 ~ 1ì‚¬ì´ ê°’ì„ ê°€ì§\n",
    "\t- EX) 2ê°€ì§€ Labelì´ ìˆë‹¤ê³  ê°€ì •, L(ì‹¤ì œ ê°’=ì •ë‹µ) âŸ¹ Bë¥¼ ì˜ˆì¸¡í•¨\n",
    "\t![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/cb6512d3-e1c1-44bd-a221-bebb4ebd0939/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T165010Z&X-Amz-Expires=86400&X-Amz-Signature=e2d6dc16737045a513ab33e2ec587f7bbadab002871024aa406e46d224d27bf0&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t- ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²½ìš° 1 : Yì˜ ì˜ˆì¸¡ì´ [0, 1]ë¡œ ë‚˜ì˜¨ ê²½ìš°\n",
    "\t\t- Yê°€  Bë¥¼ ì˜ˆì¸¡âŸ¹ ë§ì€ ì˜ˆì¸¡\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/14c9bc8f-5e24-41e5-9018-692f57ecce47/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T165750Z&X-Amz-Expires=86400&X-Amz-Signature=935b89c837db8c9b69f8d6fe9acfe3e8c0f1a5e1afffbb114a9c253c79e271d0&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t\t- cost í•¨ìˆ˜ì˜ ìµœì¢… ê°’ 0 --- good! \n",
    "\t\t\n",
    "\t- ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²½ìš° 2: Yì˜ ì˜ˆì¸¡ì´ [1, 0]ë¡œ ë‚˜ì˜¨ ê²½ìš°\n",
    "\t\t- Yê°€ Aë¥¼ ì˜ˆ\n",
    "\t\t- ì¸¡ âŸ¹ í‹€ë¦° ì˜ˆì¸¡, ê°’ì„ ì—„ì²­ í¬ê²Œ ë§Œë“¤ì–´ì„œ í‹°ëƒ„\n",
    "\t![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/c99ce0ce-c266-4049-b031-2209cd2655aa/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T170036Z&X-Amz-Expires=86400&X-Amz-Signature=b273c868939dd2a1e5163ca040a6ec5128077fb5885038699110f20727e17fc6&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t\t- ë”°ë¼ì„œ ì˜ˆì¸¡ ê°’ì´ í‹€ë¦° ê²½ìš° costê°’ì´ ë¬´í•œëŒ€ë¡œ ì¦ê°€ --- ì˜ˆì¸¡ì´ í‹€ë¦¼\n",
    "\t\t\n",
    "**ğŸ“Œ 1ê°œì˜ ë°ì´í„°ê°€ ì•„ë‹Œ nê°œì˜ datasetì´ ì£¼ì–´ì§€ëŠ” ê²½ìš° cost function**\n",
    "- ì „ì²´ì˜ ê±°ë¦¬(= D )ë¥¼ êµ¬í•œ ë’¤ nìœ¼ë¡œ ë‚˜ëˆ  í‰ê·  êµ¬í•¨\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/1a4ea154-0799-4209-b458-e4d35fadd2b8/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T171206Z&X-Amz-Expires=86400&X-Amz-Signature=39a337a9fff683db812318617b56d5e694885f454c45e66422c8b97b4c67b8a8&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "- ì´ë ‡ê²Œ **ì „ì²´ Loss(= Cost function )** ì •ì˜ ê°€ëŠ¥ \n",
    "\n",
    "## ğŸ“– softmax classifier ì˜ Gradient descent\n",
    "- ëª©ì \n",
    "\t- costë¥¼ ìµœì†Œí™” í•˜ëŠ” W ë²¡í„° ì°¾ê¸° ìœ„í•´ Gradient descentì‚¬ìš©\n",
    "\t![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/44512fb5-0efc-460a-b804-90fcbb84e487/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221015%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221015T171620Z&X-Amz-Expires=86400&X-Amz-Signature=52bfdfa976aee69b9e1ae8027bdecb45a0ebed485f81b97a18d2749552e07b88&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t- ì–‘ìƒì€ logistic functionê³¼ ë™ì¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f3549",
   "metadata": {},
   "source": [
    "# LAB 6-1 : Softmax Classifier êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "968550b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.random.set_seed(777) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d2b0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data = (8*4) -> 4ê°œì˜ feature\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "# y_data =(8*3) \n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "#convert into numpy and float : í•™ìŠµì— ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•´ numpy í˜•íƒœë¡œ ë³€í™˜\n",
    "x_data = np.asarray(x_data, dtype=np.float32)\n",
    "y_data = np.asarray(y_data, dtype=np.float32) #one-hot í‘œí˜„ ì‹œ yì˜ ê°œìˆ˜= label(class)ì˜ ê°œìˆ˜\n",
    "\n",
    "nb_classes = 3 #classì˜ ê°œìˆ˜\n",
    "#one-hot encoding ë°©ë²• - ì œì¼ í° ê°’ë§Œ 1, ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ í‘œê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c53d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight and bias setting     #4:ì…ë ¥ë˜ëŠ” ê°’ (xì˜ ê°’ = 4ê°œ) 3: ë‚˜ê°€ëŠ” ê°’ (yì˜ ê°’ - class ê°’)\n",
    "W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias') #ì¶œë ¥ê°’ - class ê°œìˆ˜\n",
    "variables = [W, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33181d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.36571955e-02 7.90162291e-03 9.78441179e-01]\n",
      " [3.92597765e-02 1.70347411e-02 9.43705440e-01]\n",
      " [3.80385250e-01 1.67723149e-01 4.51891541e-01]\n",
      " [3.23390484e-01 5.90759404e-02 6.17533624e-01]\n",
      " [3.62997366e-06 6.20727221e-08 9.99996245e-01]\n",
      " [2.62520202e-02 1.07279625e-02 9.63019967e-01]\n",
      " [1.56525093e-05 4.21802724e-07 9.99983847e-01]\n",
      " [2.94076904e-06 3.81133241e-08 9.99996960e-01]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b) #softmaxë¥¼ ì´ìš©í•´ hypothesis ì™„ì„±í•˜ê¸°\n",
    "\n",
    "print(hypothesis(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d2ace6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.9302204  0.06200533 0.00777428]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Softmax onehot test\n",
    "sample_db = [[8,2,1,4]]\n",
    "sample_db = np.asarray(sample_db, dtype=np.float32)\n",
    "\n",
    "print(hypothesis(sample_db)) #onehot encoding ì‘ë™ í…ŒìŠ¤íŠ¸ (1ê°œë§Œ 1 ë‚˜ë¨¸ì§„ 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c755d326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.07932, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cost_fn(X, Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1) #ê°€ì„¤ì„ ì´ìš©í•´ ê°€ì„¤ - ì‹¤ì œê°’ìœ¼ë¡œ costí•¨ìˆ˜\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    \n",
    "    return cost_mean\n",
    "\n",
    "print(cost_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f45f9da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#ì´ê±´ ê·¸ëƒ¥ ê¸°ìš¸ê¸° êµ¬í•˜ëŠ” with GradientTape() í…ŒìŠ¤íŠ¸ - (3,0)ì—ì„œì˜ ê¸°ìš¸ê¸°\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x # x^2\n",
    "dy_dx = g.gradient(y, x) # Will compute to 6.0\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43b1c382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[ 0.06914607, -0.6509784 ,  0.5818323 ],\n",
      "       [-1.5221257 , -1.214863  ,  2.7369885 ],\n",
      "       [-1.2473829 , -1.7611003 ,  3.0084827 ],\n",
      "       [-1.2014607 , -1.8659232 ,  3.0673835 ]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.15212914, -0.342192  ,  0.49432108], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "#Gradient Function êµ¬ì„±\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y) #loss = ì‚¬ì „ì— ì •ì˜í•œ cost function \n",
    "        grads = tape.gradient(loss, variables) #grads = gradient(loss, (W, b)ê°’)ë¡œ grad êµ¬í•˜ê¸°\n",
    "\n",
    "        return grads\n",
    "\n",
    "print(grad_fn(x_data, y_data)) #ê²½ì‚¬ ë³€í™”ëŸ‰í™•ì¸ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c16051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 0.159427\n",
      "Loss at epoch 100: 0.154003\n",
      "Loss at epoch 200: 0.148872\n",
      "Loss at epoch 300: 0.144060\n",
      "Loss at epoch 400: 0.139539\n",
      "Loss at epoch 500: 0.135282\n",
      "Loss at epoch 600: 0.131269\n",
      "Loss at epoch 700: 0.127480\n",
      "Loss at epoch 800: 0.123896\n",
      "Loss at epoch 900: 0.120502\n",
      "Loss at epoch 1000: 0.117282\n",
      "Loss at epoch 1100: 0.114226\n",
      "Loss at epoch 1200: 0.111319\n",
      "Loss at epoch 1300: 0.108553\n",
      "Loss at epoch 1400: 0.105917\n",
      "Loss at epoch 1500: 0.103402\n",
      "Loss at epoch 1600: 0.101001\n",
      "Loss at epoch 1700: 0.098706\n",
      "Loss at epoch 1800: 0.096510\n",
      "Loss at epoch 1900: 0.094407\n",
      "Loss at epoch 2000: 0.092391\n"
     ]
    }
   ],
   "source": [
    "# ìœ„ì—ì„œ êµ¬í˜„í•œ í•¨ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµëª¨ë¸ ìƒì„±í•˜ëŠ” fit í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "def fit(X, Y, epochs=2000, verbose=100):\n",
    "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1) #optimizer ì •ì˜ (learning rate = 0.1)\n",
    "    #gradient ì‚¬ìš©í•´ ê°’ ì—…ë°ì´íŠ¸ í•´ì¤Œ\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0): #100ë²ˆë§ˆë‹¤ lossê°’ ì¶œë ¥í•˜ë©° ì§„í–‰ë°©í–¥ ì˜¬ë°”ë¥¸ì§€ ì²´í¬\n",
    "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            \n",
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f374c8f",
   "metadata": {},
   "source": [
    "## Prediction\n",
    ": í•™ìŠµ í›„ Hypothesisë¥¼ í†µí•´ ì§ì ‘ ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b59c33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì‹œìš© ìƒ˜í”Œ ë°ì´í„°\n",
    "sample_data = [[2,1,3,2]] # answer_label [[0,0,1]]\n",
    "sample_data = np.asarray(sample_data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d0f3074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4.3665306e-04 4.6358623e-02 9.5320475e-01]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# ê°ê°ì˜ ì˜ˆì¸¡ê°’ì—ì„œ ê°€ì¥ ë†’ì€ ê°’ì„ ë½‘ê¸° ì˜ˆì‹œ  - one hot encoding\n",
    "# hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "a = hypothesis(sample_data)\n",
    "\n",
    "print(a)\n",
    "#ë‚˜ì˜¨ í™•ë¥  ê°’ ì¤‘ ê°€ì¥ ë†’ì€ ê°’ ì„ íƒ -> argmax\n",
    "print(tf.argmax(a, 1)) #index: 2 -> [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9613aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.9436805e-08 1.5021235e-04 9.9984980e-01]\n",
      " [4.3665289e-04 4.6358626e-02 9.5320481e-01]\n",
      " [9.5460018e-10 9.6164070e-02 9.0383589e-01]\n",
      " [2.2774371e-07 9.1306245e-01 8.6937323e-02]\n",
      " [1.6050829e-01 8.3273548e-01 6.7562237e-03]\n",
      " [8.4970415e-02 9.1502690e-01 2.6765201e-06]\n",
      " [8.2284725e-01 1.7714940e-01 3.2996732e-06]\n",
      " [9.6834594e-01 3.1654030e-02 1.4558012e-08]], shape=(8, 3), dtype=float32)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#x_dataë¡œ ì ìš©\n",
    "\n",
    "b = hypothesis(x_data)\n",
    "print(b)\n",
    "print(tf.argmax(b, 1))\n",
    "print(tf.argmax(y_data, 1)) # Yì™€ ì¼ì¹˜í•¨ - í•™ìŠµ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852ecfd",
   "metadata": {},
   "source": [
    "# LAB 6-2 : Softmax Classifier Animal Classification\n",
    ": softmax classifierì„ í†µí•´ ë™ë¬¼ ë¶„ë¥˜ í•´ë³´ê¸°\n",
    "    - softmax_cross_entropy_with_logits ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e398319b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 7)\n"
     ]
    }
   ],
   "source": [
    "#Sample dataset ì¤€ë¹„\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1] #ë§ˆì§€ë§‰ column ì œì™¸í•œ ë§Œí¼\n",
    "y_data = xy[:, -1]\n",
    "\n",
    "nb_classes = 7  # í´ë˜ìŠ¤ ìˆ«ì ì •ì˜ : 0 ~ 6 , ì´ 7ì¢…ìœ¼ë¡œ ë¶„ë¥˜ -> ì¢… ì˜ˆì¸¡\n",
    "\n",
    "#ë°ì´í„° ì „ì²˜ë¦¬\n",
    "# Yê°’ one-hot í˜•íƒœë¡œ ë³€í˜• - tf.one_hot - ì´ ê³¼ì •ì—ì„œ ì°¨ì› +1\n",
    "Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes) #astypeìœ¼ë¡œ reshape\n",
    "\n",
    "print(x_data.shape, Y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac940de7",
   "metadata": {},
   "source": [
    "- softmax classifier êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e122e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Weight and bias ì„¸íŒ…\n",
    "W = tf.Variable(tf.random.normal((16, nb_classes)), name='weight')\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
    "variables = [W, b] #weightì™€ bias ì—…ë°ì´íŠ¸í•  variable ë³„ë„ë¡œ ì§€ì •\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def logit_fn(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(logit_fn(X))\n",
    "#logit ì‚¬ìš©\n",
    "def cost_fn(X, Y):\n",
    "    logits = logit_fn(X)\n",
    "    cost_i = tf.keras.losses.categorical_crossentropy(y_true=Y, y_pred=logits, \n",
    "                                                      from_logits=True)    \n",
    "    cost = tf.reduce_mean(cost_i)    \n",
    "    return cost\n",
    "#gradient êµ¬í•¨ \n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables) #W,b ì‚¬ìš©\n",
    "        return grads\n",
    "#ë‹¨ìˆœ loss ë¿ë§Œ ì•„ë‹ˆë¼ ì •í™•ë„ë„ ê³„ì‚° (hypothesis ì‚¬ìš©ë¨)\n",
    "def prediction(X, Y):\n",
    "    pred = tf.argmax(hypothesis(X), 1)\n",
    "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb766eb",
   "metadata": {},
   "source": [
    "### Implementation - training\n",
    ": êµ¬í˜„í•œ softmax í•™ìŠµí•˜ëŠ” í•¨ìˆ˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92792e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1 Loss: 3.635028839111328, Acc: 0.1683168262243271\n",
      "Steps: 100 Loss: 0.5194157958030701, Acc: 0.7920792102813721\n",
      "Steps: 200 Loss: 0.31850090622901917, Acc: 0.9108911156654358\n",
      "Steps: 300 Loss: 0.23534879088401794, Acc: 0.9405940771102905\n",
      "Steps: 400 Loss: 0.18872138857841492, Acc: 0.9504950642585754\n",
      "Steps: 500 Loss: 0.15846037864685059, Acc: 0.9504950642585754\n",
      "Steps: 600 Loss: 0.13703757524490356, Acc: 0.9900990128517151\n",
      "Steps: 700 Loss: 0.12098980695009232, Acc: 0.9900990128517151\n",
      "Steps: 800 Loss: 0.10847964882850647, Acc: 1.0\n",
      "Steps: 900 Loss: 0.09843041002750397, Acc: 1.0\n",
      "Steps: 1000 Loss: 0.09016557037830353, Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=1000, verbose=100):\n",
    "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            acc = prediction(X, Y).numpy() #prediction í•¨ìˆ˜ë¥¼ í†µí•´ ì •í™•ë„ê¹Œì§€ ì¶œë ¥\n",
    "            loss = cost_fn(X, Y).numpy() \n",
    "            print('Steps: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))\n",
    "\n",
    "fit(x_data, Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d412a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
