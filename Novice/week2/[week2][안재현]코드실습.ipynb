{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7de878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "061573ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  172997.0000\n",
      "   50 |    1925.2883\n",
      "  100 |      27.0618\n",
      "  150 |       5.9909\n",
      "  200 |       5.7490\n",
      "  250 |       5.7382\n",
      "  300 |       5.7301\n",
      "  350 |       5.7219\n",
      "  400 |       5.7139\n",
      "  450 |       5.7058\n",
      "  500 |       5.6977\n",
      "  550 |       5.6897\n",
      "  600 |       5.6816\n",
      "  650 |       5.6737\n",
      "  700 |       5.6657\n",
      "  750 |       5.6578\n",
      "  800 |       5.6498\n",
      "  850 |       5.6419\n",
      "  900 |       5.6341\n",
      "  950 |       5.6262\n",
      " 1000 |       5.6183\n"
     ]
    }
   ],
   "source": [
    "x1 = [73., 93., 89., 96., 73.]\n",
    "x2 = [80., 88., 91., 98., 66.]\n",
    "x3 = [75., 93., 90., 100., 70.]\n",
    "Y = [152., 185., 180., 196., 142.]\n",
    "\n",
    "\n",
    "w1 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "w2 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "w3 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "for i in range(1000+1):\n",
    "    # gradient descent 경사하강을 통해서 weight값을 지속적으로 업데이트 하는 과정\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1*x1 + w2*x2 + w3*x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "        \n",
    "        w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "        \n",
    "        w1.assign_sub(learning_rate * w1_grad) # 기울기 update\n",
    "        w2.assign_sub(learning_rate * w2_grad)\n",
    "        w3.assign_sub(learning_rate * w3_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(\"{:5} | {:12.4f}\".format(i, cost.numpy())) # 50번 마다 cost 값 출력\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14021d27",
   "metadata": {},
   "source": [
    "# matrix로 표현 H(x)=XW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6202d43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  5076.4795\n",
      "  100 |     2.3076\n",
      "  200 |     1.6821\n",
      "  300 |     1.6814\n",
      "  400 |     1.6807\n",
      "  500 |     1.6800\n",
      "  600 |     1.6793\n",
      "  700 |     1.6786\n",
      "  800 |     1.6779\n",
      "  900 |     1.6772\n",
      " 1000 |     1.6765\n",
      " 1100 |     1.6758\n",
      " 1200 |     1.6751\n",
      " 1300 |     1.6744\n",
      " 1400 |     1.6737\n",
      " 1500 |     1.6730\n",
      " 1600 |     1.6723\n",
      " 1700 |     1.6716\n",
      " 1800 |     1.6709\n",
      " 1900 |     1.6702\n",
      " 2000 |     1.6695\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    [73.,80.,75.,152.],\n",
    "    [93.,88.,93.,185.],\n",
    "    [89.,91.,90.,180.],\n",
    "    [96.,98.,100.,196.],\n",
    "    [73.,66.,70.,142.]\n",
    "], dtype=np.float32)\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, [-1]]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([3, 1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "\n",
    "def predict(x):\n",
    "    return tf.matmul(X, W)+b # XW + b\n",
    "\n",
    "n_epochs = 2000\n",
    "\n",
    "for i in range(n_epochs+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(predict(X)-y)))\n",
    "        \n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        \n",
    "        # update parameter W, b\n",
    "        \n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"{:5} | {:10.4f}\".format(i, cost.numpy())) # 100번 마다 cost 값 출력\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330142a9",
   "metadata": {},
   "source": [
    "matrix를 사용하지 않으면 변수를 전부 다 적어주어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4191a",
   "metadata": {},
   "source": [
    "다변수선형회귀에서 matrix를 이용했을 때의 이점.\n",
    "\n",
    "wieght와 x를 한줄로 표현할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6891ed3",
   "metadata": {},
   "source": [
    "# Logistic Regression Lab05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1f3602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "x_train = [[1., 2.],\n",
    "          [2., 3.],\n",
    "          [3., 1.],\n",
    "          [4., 3.],\n",
    "          [5., 3.],\n",
    "          [6., 2.]]\n",
    "y_train = [[0.],\n",
    "          [0.],\n",
    "          [0.],\n",
    "          [1.],\n",
    "          [1.],\n",
    "          [1.]]\n",
    "\n",
    "x_test = [[5.,2.]]\n",
    "y_test = [[1.]]\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train)).repeat()\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "\n",
    "features = tf.cast(features, tf.float32)\n",
    "labels = tf.cast(labels, tf.float32)\n",
    "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')\n",
    "\n",
    "\n",
    "                                             \n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90c8f8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = [x[0] for x in x_train]\n",
    "x2 = [x[1] for x in x_train]\n",
    "\n",
    "colors = [int(y[0] % 3) for y in y_train]\n",
    "plt.scatter(x1,x2, c=colors , marker='^')\n",
    "plt.scatter(x_test[0][0],x_test[0][1], c=\"red\")\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d88618d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "452c10e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid 함수를 이용해 hypothesis를 구하고\n",
    "\n",
    "hypothesis  = tf.div(1., 1. + tf.exp(tf.matmul(features, W) + b)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf313b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(labels * tf.log(hypothesis) + (1 - labels) * tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5734d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1cc3a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f355d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features):\n",
    "    hypothesis = tf.div(1., 1.+tf.exp(tf.matmul(features, W) +b))\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b2c9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn (features, labels):\n",
    "    hypothesis = logistic_regression(features)\n",
    "    cost = -tf.reduce_mean(labels * tf.log(loss_fn(hypothesis)) + (1-labels)*tf.log(1-hypothesis))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "633a1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ㅎ\n",
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(hypothesis, labels)\n",
    "    return tape.gradient(loss_value, [W,b])\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53f98bf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfe\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m features, labels \u001b[38;5;129;01min\u001b[39;00m tfe\u001b[38;5;241m.\u001b[39mIterator(dataset):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels in tfe.Iterator(dataset):\n",
    "        grads = grad(logistic_regression(features), features, labels)\n",
    "        optimizer.apply_gradient(grads_and_vars = zip(grads, [W,b]))\n",
    "        \n",
    "        if step % 100 ==0:\n",
    "            print(\"Iter : {}, Loss: {:..4f}\".format(step, loss_fn(logistic_regression(features), labels)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c35083c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.6931\n",
      "Iter: 1000, Loss: 0.4145\n",
      "Iter: 2000, Loss: 0.3496\n",
      "Iter: 3000, Loss: 0.3014\n",
      "Iter: 4000, Loss: 0.2636\n",
      "Iter: 5000, Loss: 0.2336\n",
      "Iter: 6000, Loss: 0.2094\n",
      "Iter: 7000, Loss: 0.1896\n",
      "Iter: 8000, Loss: 0.1731\n",
      "Iter: 9000, Loss: 0.1592\n",
      "Iter: 10000, Loss: 0.1474\n",
      "\n",
      "Hypothesis:  [[0.02987642]\n",
      " [0.1576593 ]\n",
      " [0.30070737]\n",
      " [0.78328896]\n",
      " [0.9407705 ]\n",
      " [0.98057085]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1\n",
      "\n",
      "Test Data : [[5.0, 2.0]], Predict : [[1.]]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(EPOCHS):\n",
    "        sess.run(iter.initializer)\n",
    "        _, loss_value = sess.run([train, cost])\n",
    "        if step % 1000 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_value))\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy])\n",
    "    \n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n",
    "    print(\"\\nTest Data : {}, Predict : {}\".format(x_test, sess.run(predicted, feed_dict={features: x_test})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5f072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{}
