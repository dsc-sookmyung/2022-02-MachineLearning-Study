{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b3c107",
   "metadata": {},
   "source": [
    "[강의 정리 링크](https://sooking87.github.io/gdsc%20ml/gdsc-ml-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff4886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   21359.2578\n",
      "   50 |     245.4268\n",
      "  100 |      11.1252\n",
      "  150 |       8.5033\n",
      "  200 |       8.4521\n",
      "  250 |       8.4294\n",
      "  300 |       8.4072\n",
      "  350 |       8.3851\n",
      "  400 |       8.3629\n",
      "  450 |       8.3410\n",
      "  500 |       8.3189\n",
      "  550 |       8.2970\n",
      "  600 |       8.2751\n",
      "  650 |       8.2533\n",
      "  700 |       8.2315\n",
      "  750 |       8.2098\n",
      "  800 |       8.1882\n",
      "  850 |       8.1667\n",
      "  900 |       8.1451\n",
      "  950 |       8.1236\n",
      " 1000 |       8.1022\n"
     ]
    }
   ],
   "source": [
    "# Multi variable linear regression with no matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "# data and label\n",
    "x1 = [73., 93., 89., 96., 73]\n",
    "x2 = [80., 88.,91., 98., 66.]\n",
    "x3 = [75., 93., 90., 100., 70.]\n",
    "Y = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# weights\n",
    "w1 = tf.Variable(tf.random.normal([1]))\n",
    "w2 = tf.Variable(tf.random.normal([1]))\n",
    "w3 = tf.Variable(tf.random.normal([1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
    "\n",
    "for i in range(1000+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    # calculates the gradients of the cost    \n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    # update w1, w2, w3 and b\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 33753.4961\n",
      "  100 |    14.9227\n",
      "  200 |    10.7120\n",
      "  300 |    10.6551\n",
      "  400 |    10.5989\n",
      "  500 |    10.5431\n",
      "  600 |    10.4877\n",
      "  700 |    10.4324\n",
      "  800 |    10.3776\n",
      "  900 |    10.3230\n",
      " 1000 |    10.2687\n",
      " 1100 |    10.2147\n",
      " 1200 |    10.1609\n",
      " 1300 |    10.1075\n",
      " 1400 |    10.0543\n",
      " 1500 |    10.0015\n",
      " 1600 |     9.9489\n",
      " 1700 |     9.8966\n",
      " 1800 |     9.8447\n",
      " 1900 |     9.7929\n",
      " 2000 |     9.7415\n"
     ]
    }
   ],
   "source": [
    "# Multi variable linear regression with matrix\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([\n",
    "    # x1, x2, x3, y\n",
    "    [73., 80., 75., 152.],\n",
    "    [93., 88., 93., 185.],\n",
    "    [89., 91., 90., 180.],\n",
    "    [96., 98., 100., 196.],\n",
    "    [73., 66., 70., 142.],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# slice data\n",
    "X = data[:, :-1]\n",
    "y = data[:, [-1]]\n",
    "\n",
    "# 가중치의 경우 칼럼이 3개(x1, x2, x3)니까 행은 3개가 필요하고, 결과값은 1개니까 열 개수는 1개가 필요하다. \n",
    "W = tf.Variable(tf.random.normal([3, 1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_data = 0.000001\n",
    "\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs + 1):\n",
    "    # record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
    "        \n",
    "    # calculates the gradients of the loss \n",
    "    # cost 함수에 대해서 변수들에 대한 개별 기울기값을 튜플로 반환\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    # updates parameters (W and b)\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))\n",
    "       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4800d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(777)  # for reproducibility\n",
    "print(tf.__version__)\n",
    "\n",
    "# Data\n",
    "x_train = [[1., 2.],\n",
    "          [2., 3.],\n",
    "          [3., 1.],\n",
    "          [4., 3.],\n",
    "          [5., 3.],\n",
    "          [6., 2.]]\n",
    "y_train = [[0.],\n",
    "          [0.],\n",
    "          [0.],\n",
    "          [1.],\n",
    "          [1.],\n",
    "          [1.]]\n",
    "\n",
    "x_test = [[5.,2.]]\n",
    "y_test = [[1.]]\n",
    "\n",
    "\n",
    "x1 = [x[0] for x in x_train]\n",
    "x2 = [x[1] for x in x_train]\n",
    "\n",
    "colors = [int(y[0] % 3) for y in y_train]\n",
    "plt.scatter(x1,x2, c=colors , marker='^')\n",
    "plt.scatter(x_test[0][0],x_test[0][1], c=\"red\")\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72765390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>\n",
      "Iter: 0, Loss: 0.6874\n",
      "Iter: 100, Loss: 0.5776\n",
      "Iter: 200, Loss: 0.5349\n",
      "Iter: 300, Loss: 0.5054\n",
      "Iter: 400, Loss: 0.4838\n",
      "Iter: 500, Loss: 0.4671\n",
      "Iter: 600, Loss: 0.4535\n",
      "Iter: 700, Loss: 0.4420\n",
      "Iter: 800, Loss: 0.4319\n",
      "Iter: 900, Loss: 0.4228\n",
      "Iter: 1000, Loss: 0.4144\n",
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow data API를 통해 학습시킬 값들을 담는다 (Batch Size는 한번에 학습시킬 Size로 정한다)\n",
    "# features,labels는 실재 학습에 쓰일 Data (연산을 위해 Type를 맞춰준다)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat()\n",
    "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')\n",
    "\n",
    "# Sigmoid 함수를 가설로 선언합니다\n",
    "def logistic_regression(features):\n",
    "    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
    "    return hypothesis\n",
    "\n",
    "# 비용 함수를 정의합니다. \n",
    "def loss_fn(hypothesis, features, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "# GradientDescentOptimizer에서 learning_rate를 설정해서 optimizer를 선언한다.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# GradientTape를 통해 경사값을 계산합니다.\n",
    "def grad(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(logistic_regression(features),features, labels)\n",
    "    return tape.gradient(loss_value, [W,b])\n",
    "\n",
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels  in iter(dataset):\n",
    "        grads = grad(features, labels)\n",
    "        # 실제 optimizer.apply_gradients를 통해서 실제모델인 [W,b]가 update되서 모델이 만들어지게된다.\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
    "\n",
    "# 추론한 값은 0.5를 기준(Sigmoid 그래프 참조)로 0과 1의 값을 리턴합니다.\n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
    "    return accuracy\n",
    "           \n",
    "test_acc = accuracy_fn(logistic_regression(x_test),y_test)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "35e0334895c7257ffa902a82750dbe09ec85e290cbc1321a3819ebf1c9410545"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
