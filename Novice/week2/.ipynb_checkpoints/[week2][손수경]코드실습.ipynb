{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b3c107",
   "metadata": {},
   "source": [
    "[강의 정리 링크](https://sooking87.github.io/gdsc%20ml/gdsc-ml-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff4886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   21359.2578\n",
      "   50 |     245.4268\n",
      "  100 |      11.1252\n",
      "  150 |       8.5033\n",
      "  200 |       8.4521\n",
      "  250 |       8.4294\n",
      "  300 |       8.4072\n",
      "  350 |       8.3851\n",
      "  400 |       8.3629\n",
      "  450 |       8.3410\n",
      "  500 |       8.3189\n",
      "  550 |       8.2970\n",
      "  600 |       8.2751\n",
      "  650 |       8.2533\n",
      "  700 |       8.2315\n",
      "  750 |       8.2098\n",
      "  800 |       8.1882\n",
      "  850 |       8.1667\n",
      "  900 |       8.1451\n",
      "  950 |       8.1236\n",
      " 1000 |       8.1022\n"
     ]
    }
   ],
   "source": [
    "# Multi variable linear regression with no matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "# data and label\n",
    "x1 = [73., 93., 89., 96., 73]\n",
    "x2 = [80., 88.,91., 98., 66.]\n",
    "x3 = [75., 93., 90., 100., 70.]\n",
    "Y = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# weights\n",
    "w1 = tf.Variable(tf.random.normal([1]))\n",
    "w2 = tf.Variable(tf.random.normal([1]))\n",
    "w3 = tf.Variable(tf.random.normal([1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
    "\n",
    "for i in range(1000+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    # calculates the gradients of the cost    \n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    # update w1, w2, w3 and b\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 33753.4961\n",
      "  100 |    14.9227\n",
      "  200 |    10.7120\n",
      "  300 |    10.6551\n",
      "  400 |    10.5989\n",
      "  500 |    10.5431\n",
      "  600 |    10.4877\n",
      "  700 |    10.4324\n",
      "  800 |    10.3776\n",
      "  900 |    10.3230\n",
      " 1000 |    10.2687\n",
      " 1100 |    10.2147\n",
      " 1200 |    10.1609\n",
      " 1300 |    10.1075\n",
      " 1400 |    10.0543\n",
      " 1500 |    10.0015\n",
      " 1600 |     9.9489\n",
      " 1700 |     9.8966\n",
      " 1800 |     9.8447\n",
      " 1900 |     9.7929\n",
      " 2000 |     9.7415\n"
     ]
    }
   ],
   "source": [
    "# Multi variable linear regression with matrix\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([\n",
    "    # x1, x2, x3, y\n",
    "    [73., 80., 75., 152.],\n",
    "    [93., 88., 93., 185.],\n",
    "    [89., 91., 90., 180.],\n",
    "    [96., 98., 100., 196.],\n",
    "    [73., 66., 70., 142.],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# slice data\n",
    "X = data[:, :-1]\n",
    "y = data[:, [-1]]\n",
    "\n",
    "# 가중치의 경우 칼럼이 3개(x1, x2, x3)니까 행은 3개가 필요하고, 결과값은 1개니까 열 개수는 1개가 필요하다. \n",
    "W = tf.Variable(tf.random.normal([3, 1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_data = 0.000001\n",
    "\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs + 1):\n",
    "    # record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
    "        \n",
    "    # calculates the gradients of the loss \n",
    "    # cost 함수에 대해서 변수들에 대한 개별 기울기값을 튜플로 반환\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    # updates parameters (W and b)\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))\n",
    "       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64e9614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow.contrib (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow.contrib\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e47b244",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\STUDY\\4학기\\2022-02-MachineLearning-Study\\Novice\\week2\\.ipynb_checkpoints\\[week2][손수경]코드실습.ipynb 셀 5\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#X10sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m3000\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#X10sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#X10sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39mfor\u001b[39;00m features, labels \u001b[39min\u001b[39;00m \u001b[39miter\u001b[39;49m(dataset):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#X10sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         hypothesis \u001b[39m=\u001b[39m logistic_regression(features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         grads \u001b[39m=\u001b[39m grad(hypothesis, features, labels)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:496\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39mOwnedIterator(\u001b[39mself\u001b[39m)\n\u001b[0;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    497\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: `tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function."
     ]
    }
   ],
   "source": [
    "# 유튭 댓글\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "x_train = np.array([\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 1],\n",
    "    [4, 3],\n",
    "    [5, 3],\n",
    "    [6, 2]], dtype=np.float32)\n",
    "y_train = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [1]], dtype=np.float32)\n",
    "\n",
    "x_test = np.array([[5, 2]], dtype=np.float32)\n",
    "y_test = np.array([[1]], dtype=np.float32)\n",
    "\n",
    "# tf.data.Dataset 파이프라인을 이용하여 값을 입력\n",
    "# from_tensor_slices 클래스 매서드를 사용하면 리스트, 넘파이, 텐서플로 자료형에서 데이터셋을 만들 수 있음\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n",
    "W = tf.Variable(tf.zeros([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')\n",
    "\n",
    "# 원소의 자료구조 반환\n",
    "dataset.element_spec\n",
    "\n",
    "def logistic_regression(features):\n",
    "    hypothesis = tf.sigmoid(tf.matmul(features, W) + b)\n",
    "    return hypothesis\n",
    "\n",
    "\n",
    "def loss_fn(features, labels):\n",
    "    hypothesis = logistic_regression(features)\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(features, labels)\n",
    "    return tape.gradient(loss_value, [W,b])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "EPOCHS = 3000\n",
    "\n",
    "for step in range(EPOCHS + 1):\n",
    "    for features, labels in iter(dataset):\n",
    "        hypothesis = logistic_regression(features)\n",
    "        grads = grad(hypothesis, features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, [W,b]))\n",
    "        if step % 300 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(features, labels)))\n",
    "            # print(hypothesis)\n",
    "            \n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
    "    return accuracy\n",
    "\n",
    "test_acc = accuracy_fn(logistic_regression(x_test), y_test)\n",
    "print('Accuracy: {}%'.format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5834cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow.compat.v1 (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow.compat.v1\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow.compat.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2292f7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.4371388\n",
      "200 0.69732946\n",
      "400 0.63960725\n",
      "600 0.60207886\n",
      "800 0.57347494\n",
      "1000 0.54938275\n",
      "1200 0.52789503\n",
      "1400 0.50811946\n",
      "1600 0.48961148\n",
      "1800 0.47213662\n",
      "2000 0.45556316\n",
      "2200 0.43981054\n",
      "2400 0.4248244\n",
      "2600 0.4105635\n",
      "2800 0.39699268\n",
      "3000 0.38408002\n",
      "3200 0.3717951\n",
      "3400 0.36010852\n",
      "3600 0.34899107\n",
      "3800 0.33841452\n",
      "4000 0.328351\n",
      "4200 0.3187736\n",
      "4400 0.3096563\n",
      "4600 0.30097374\n",
      "4800 0.2927021\n",
      "5000 0.28481847\n",
      "5200 0.27730104\n",
      "5400 0.27012888\n",
      "5600 0.2632827\n",
      "5800 0.25674376\n",
      "6000 0.25049475\n",
      "6200 0.24451935\n",
      "6400 0.238802\n",
      "6600 0.23332839\n",
      "6800 0.2280848\n",
      "7000 0.2230586\n",
      "7200 0.21823776\n",
      "7400 0.21361105\n",
      "7600 0.20916803\n",
      "7800 0.20489876\n",
      "8000 0.20079412\n",
      "8200 0.19684546\n",
      "8400 0.19304447\n",
      "8600 0.1893837\n",
      "8800 0.18585597\n",
      "9000 0.18245459\n",
      "9200 0.17917325\n",
      "9400 0.17600608\n",
      "9600 0.17294745\n",
      "9800 0.16999215\n",
      "10000 0.16713524\n",
      "\n",
      "Hypothesis:\n",
      " [[0.03849334]\n",
      " [0.16818573]\n",
      " [0.34004897]\n",
      " [0.76573867]\n",
      " [0.92929274]\n",
      " [0.9767822 ]] \n",
      "Correct(Y):\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "# 구글링 결과 \n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0],[0],[0],[1],[1],[1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# sigmoid함수를 input summation 해주는 활성함수 적용\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cross entropy라는 Cost function정의\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1- hypothesis))\n",
    "# SGD 적용해서 minimize cost\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# 0.5값 이상이면 True(1)로 예측되도록 함\n",
    "# cast함수: float형으로 출력되는 hypothesis값이 0.5보다 크면 정수 1(True), 작으면 정수 0(False)으로 변환\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "# 예측값과 실제값 비교해서 Boolean으로 반환 후, 1(True)값들의 평균을 내어서 accuracy 출력\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y),\n",
    "                                 dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train],\n",
    "                              feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "    \n",
    "    # 위에서 역전파 학습으로 cost를 최소하하도록 학습한 후 마지막 업데이트 상태에서 forwarding한 후 예측된 확률값, 예측된 클래스값, 정확도 출력\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                      feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis:\\n\", h, \"\\nCorrect(Y):\\n\", c,\"\\nAccuracy:\\n\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64b91559",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loss_fn() missing 1 required positional argument: 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\STUDY\\4학기\\2022-02-MachineLearning-Study\\Novice\\week2\\.ipynb_checkpoints\\[week2][손수경]코드실습.ipynb 셀 6\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m    \u001b[39mfor\u001b[39;00m features, labels \u001b[39min\u001b[39;00m \u001b[39miter\u001b[39m(dataset):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         grads \u001b[39m=\u001b[39m grad(logistic_regression(features), features, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mapply_gradients(grads_and_vars\u001b[39m=\u001b[39m\u001b[39mzip\u001b[39m(grads, [W,b]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m :\n",
      "\u001b[1;32mc:\\STUDY\\4학기\\2022-02-MachineLearning-Study\\Novice\\week2\\.ipynb_checkpoints\\[week2][손수경]코드실습.ipynb 셀 6\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(hypothesis, features, labels)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrad\u001b[39m(hypothesis, features, labels):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         loss_value \u001b[39m=\u001b[39m loss_fn(hypothesis, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tape\u001b[39m.\u001b[39mgradient(loss_value, [W, b])\n",
      "\u001b[1;32mc:\\STUDY\\4학기\\2022-02-MachineLearning-Study\\Novice\\week2\\.ipynb_checkpoints\\[week2][손수경]코드실습.ipynb 셀 6\u001b[0m in \u001b[0;36mloss_fn\u001b[1;34m(hypothesis, labels)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_fn\u001b[39m(hypothesis, labels):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     hypothesis \u001b[39m=\u001b[39m logistic_regression(features)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     cost \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtf\u001b[39m.\u001b[39mreduce_mean(labels \u001b[39m*\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mlog(loss_fn(hypothesis)) \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m labels) \u001b[39m*\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m hypothesis))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/STUDY/4%ED%95%99%EA%B8%B0/2022-02-MachineLearning-Study/Novice/week2/.ipynb_checkpoints/%5Bweek2%5D%5B%EC%86%90%EC%88%98%EA%B2%BD%5D%EC%BD%94%EB%93%9C%EC%8B%A4%EC%8A%B5.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cost\n",
      "\u001b[1;31mTypeError\u001b[0m: loss_fn() missing 1 required positional argument: 'labels'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_train = np.array([[1., 2.],\n",
    "           [2., 3.],\n",
    "           [3., 1.],\n",
    "           [4., 3.],\n",
    "           [5., 3.],\n",
    "           [6., 2.]], dtype=np.float32)\n",
    "y_train = np.array([[0.],\n",
    "           [0.],\n",
    "           [0.],\n",
    "           [1.],\n",
    "           [1.],\n",
    "           [1.]], dtype=np.float32)\n",
    "x_test = np.array([[5., 2.]], dtype=np.float32)\n",
    "y_test = np.array([[1.]], dtype=np.float32)\n",
    "\n",
    "# tf.data.Dataset 파이프라인을 이용하여 값을 입력\n",
    "# from_tensor_slices 클래스 매서드를 사용하면 리스트, 넘파이, 텐서플로 자료형에서 데이터셋을 만들 수 있음\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n",
    "W = tf.Variable(tf.zeros([2, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.zeros([1]), name=\"bias\")\n",
    "\n",
    "def logistic_regression(features):\n",
    "    hypothesis = tf.divide(1., 1. + tf.exp(tf.matmul(features, W)+ b))\n",
    "    return hypothesis\n",
    "\n",
    "def loss_fn(hypothesis, labels):\n",
    "    hypothesis = logistic_regression(features)\n",
    "    cost = -tf.reduce_mean(labels * tf.mat  h.log(loss_fn(hypothesis)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(hypothesis, labels)\n",
    "    return tape.gradient(loss_value, [W, b])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "EPOCHS = 1001\n",
    "for step in range(EPOCHS):\n",
    "   for features, labels in iter(dataset):\n",
    "        grads = grad(logistic_regression(features), features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, [W,b]))\n",
    "        if step % 100 == 0 :\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features), labels)))\n",
    "            \n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.int32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "test_acc = accuracy_fn(logistic_regression(x_test), y_test)\n",
    "print('Accuracy: {}%'.format(test_acc * 100))\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
