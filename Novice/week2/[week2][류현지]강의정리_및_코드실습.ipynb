{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b253d27",
   "metadata": {},
   "source": [
    "# Lec 4. Multi variable linear regression\n",
    "**📌 변수가 여러 개일 때 Hypothesis / Cost function**\n",
    "- 변수의 개수가 증가할 수록, 수식으로 표현하는데 한계 → ***Matrix***  이용\n",
    "![](https://1.bp.blogspot.com/-Jp5xumLwAz8/XFASYqRzIqI/AAAAAAAAA5s/It-uq9d6eJsdIVZWz-8W95yYOy6fhhsrwCLcBGAs/s400/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA%2B2019-01-29%2B%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE%2B5.44.04.png)\n",
    "- Tensor flow에선 → XW로 표현 (행렬의 곱 때문에)\n",
    "- 따라서 다변수 hypothesis를 각 행렬(X, W)의 곱으로 표현 이 가능하다. (X가 앞에 옴) ⇒ H(X) = XW\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/7b581b07-d313-42b2-83d9-5337ffb4fc6e/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221008%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221008T113240Z&X-Amz-Expires=86400&X-Amz-Signature=0ea20a5b9600b02baea487e512b550dd4968c6319c6c796c47485a5c8d9bf26a&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t- X Matrix\n",
    "\t\t- Column 수 :  feature 수\n",
    "\t\t- Row 수 : instance 수\n",
    "\n",
    "**📌 Matrix를 사용해 표현하는 것의 장점**\n",
    "- instance 개수, feature 개수 상관없이  ***H(X) = XW*** 로 동일한 표현 사용 가능함.\n",
    "- instance 개수 상관X (→ 내적과 무관)\n",
    "\n",
    "**📌 Weight matrix의 크기 결정?**\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/b57e09c6-f0f8-4b93-98be-2f9c68df18f5/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221008%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221008T114336Z&X-Amz-Expires=86400&X-Amz-Signature=7bef43a0befdf759d9b96acdd2ebe2afb6a4371f88515ad04727868f224bd8eb&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t- Row = 입력 데이터(X)의 Column (feature) (입력 개수)\n",
    "\t- Column = 출력(H)의 Column\n",
    "\n",
    "> ∴ **Weight의 크기는 입력/출력 개수와 무관하다!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1262c",
   "metadata": {},
   "source": [
    "# **Lec 5. Logistic Regression**\n",
    "\n",
    "**📖 Logistic Regression 개념적 접근**\n",
    "\n",
    " **Binary Classificaiton**\n",
    " - 2가지 값으로만 표현됨 - 0 (positive), 1(negative) \n",
    " - 이것을 토대로 Logistic Regression 모델 생성\n",
    "\n",
    "**Logistic vs. Linear**\n",
    "- Logistic을 적용하기 위한 data\n",
    "\t- 2가지 case로 분리 가능한 data\n",
    "\t- 셀 수 있고 흩어져 있음 (Discrete ; Counted)\n",
    "\t- Ex. 신발 사이즈\n",
    "\t∴ Binary 값 사용\n",
    "\n",
    "- Linear을 적용하기 위한 data\n",
    "\t- 데이터들이 연속적임 (Continuous ; Measured)\n",
    "\t- 이어지는 값들을 예측 가능\n",
    "\t- Ex. 시간, 몸무게\n",
    "\t∴ Numeric 값 사용 (수치형 데이터)\n",
    "\n",
    "**📖 Logistic Regression 의 Hypothesis 표현**\n",
    "\n",
    "**[예시] Study Hour → Test Pass/Fail 예측하기**\n",
    "\n",
    "- Linear Regression을 적용 했을 경우\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/43a9da37-e1bd-47fd-99fd-b2c511cb2fe8/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221008%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221008T132555Z&X-Amz-Expires=86400&X-Amz-Signature=184eab361a314c299b8a451b398515f227432393c25008467b2ecd205bba98e1&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t- 결과값으로 연속적인 수치 값으로 데이터가 나옴\n",
    "\t- But 원하는 값 ⇒  Binary Classification에 대한 결과값이 필요함 (Pass냐 Fail이냐)\n",
    "\n",
    "- Logistic Regression \n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/2f70d3e4-38c0-474e-8665-e9dc39ac4b2b/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221008%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221008T155345Z&X-Amz-Expires=86400&X-Amz-Signature=126726eaddbdbc5ba05e953042a4663b89eef30ba11bb4088868886ab43707f3&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\t- linear function의 output을 넣은 결과 = 0 or 1\n",
    "\t\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5efb277c-bb3a-447a-89f5-c0a901c46f0d/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221008%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221008T155505Z&X-Amz-Expires=86400&X-Amz-Signature=b773f5efec0d508a15b8b8a7495bd8170a51dfee55b31fb155bd376da089d427&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\n",
    "- 뉴런 네트워크를 만들기 위한 하나의 컴포넌트이기도 함\n",
    "> **X → Linear 한 값 → 0 ~1 구간 값 → decision boundary → 0과 1의 값으로 도출**\n",
    "\n",
    "**📖 Sigmoid (Logistic) function  [g(z)]**\n",
    "\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4b4ac69b-2a16-4501-81af-ea434cdeae95/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221008%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221008T164109Z&X-Amz-Expires=86400&X-Amz-Signature=3ca6a104249eb80399c11865bc24eb134827f387d096f010d1f9a7a6f1481080&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "- z(input)은 real number(수치형 값)\n",
    "- z  증가 ↑ ∝ e^(-x) ↓ , g(z) ≈ 1\n",
    "- z  증가 ↓ ∝ e^(-x) ↑ , g(z) ≈ 0\n",
    "- linear regression으로 나온 실수 값을 sigmoid func을 통해 0, 1 사이의 값으로 도출 가능함.\n",
    "\n",
    "**📌 Decision Boundary**\n",
    "- 0.5를 기준으로, 보다 크면 1/작으면 0으로 처리\n",
    "- Sigmoid function의 output은 0 ≤ y ≤ 1 범위의 numerical value ∴ binary하게 처리 필요 (→ Decision Boundary)\n",
    "- decision boundary function 표현은 Linear/Non-linear 종류 모두 가능\t\n",
    "![Untitled.png (267×246)](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/a7902d10-0150-413b-a000-32b1d95a9faf/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221008%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221008T165601Z&X-Amz-Expires=86400&X-Amz-Signature=1e20e1228139a522269c96941fca2cf9204fe18cbae981db79c53ccd47b57200&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/85c011c8-cd1c-4b6a-b4f5-8dbe38a551e9/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221008%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221008T164955Z&X-Amz-Expires=86400&X-Amz-Signature=69d884badcc4dd5a0f38a461b8297c7d7f5b7c4b22e24189d962926f904b4a94&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject) \n",
    "\n",
    "    - Linear : x2 = -x1 + 3으로 직선 그래프가 그려지고, 위<-> 아래로 분할됨\n",
    "\t\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/f1ae3fa8-58fa-4692-a899-3f42d2703524/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221008%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221008T165431Z&X-Amz-Expires=86400&X-Amz-Signature=7dfec5309453449b7c7b25ae70973d1a7a908e492887443867ceacce3c07db60&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/0410d0ff-26f2-43fc-9516-98262699a16d/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221008%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221008T165452Z&X-Amz-Expires=86400&X-Amz-Signature=e734cebc22517d3892b81f09b8b020fb977bc5168d40eaaf638ed0241f53226c&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "\n",
    "    - Non - linear : 원 그래프가 그려지고 1이 decision boundary 결정 수가 된다.\n",
    "    \n",
    "## **📌 Cost Function**\n",
    "- Cost = 원하는 이상적인 model 값 - 최초에 만들어진 model 값\n",
    "- 따라서 Cost를 줄여야 원하는 모델값을 만들어 낼 수 있음 (cost = 0)\n",
    "\t- 데이터 학습을 통해 최적의 parameter값 찾기\n",
    "\t- Cost 수식\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/131f35ab-7bcb-48ea-b971-9a699224b32d/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221009%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221009T152633Z&X-Amz-Expires=86400&X-Amz-Signature=03962373877eae5ebb4f821e1b6d0e4fdbb280dafcec3c06bf04e3b43ae17bc5&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "- 구현 시 input = hypothesis , labels \n",
    "\t- Cost = hypothesis - labels\n",
    "\t- out put = cost 값\n",
    "\t- cost가 0에 가까울수록 분류를 잘 하는 것\n",
    "- Cost 값을 최소화하기 위한 과정\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/ab00cfc9-19df-4fde-974a-febdd9e408d1/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221009%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221009T153057Z&X-Amz-Expires=86400&X-Amz-Signature=ec16d3155fdfe48279ab127cea32194ab158ea0f2000829ef62b5c1bec2e2183&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/3054f987-d46d-451b-8baa-b1e4b0380881/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221009%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221009T154332Z&X-Amz-Expires=86400&X-Amz-Signature=456eee83aeb13045665ac06f0954a2c29f5d4b72afcb13f4ad4bed5822b5a76b&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "- logistic regression의 1/0의 값에 대한 cost 함수임\n",
    "- y = 1 → 실제 원하는 값이 1인 경우, 가설 값도 1이여야 cost = 0\n",
    "\t- 반대로 가설 값 (실제로 나오는 값)이 0이면 → cost가 무한에 가깝게 증가\n",
    "\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/8563d26a-e2bc-46c4-8995-07a5734e8485/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221009%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221009T154402Z&X-Amz-Expires=86400&X-Amz-Signature=f581239ee773c897ca734c39f290132d2835b772a19bfc7ab78cf9b658e86150&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "> 위 두 가지 경우를 합치면 이상적인 convex 구조를 가진 cost function 도출\n",
    "\n",
    "**📌 Optimization: Cost function을 최소화 할 수 있는 방법?**\n",
    "\n",
    "- Cost function에서 순간 속도 기울기가 0에 가까운 θ 찾기\n",
    "![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/ba338778-f4c6-419e-929d-078f774a7741/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221009%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221009T154850Z&X-Amz-Expires=86400&X-Amz-Signature=1ff72beaf51ed4c75f99f1a2506f8c91ace406927f92806c74d7e84397acda10&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
    "- Gradient, learning rate, model 값(W, b)를 통해 최적의 값(optimizer 최적화 하는 값)을 찾아낼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da624b7",
   "metadata": {},
   "source": [
    "# Lec 4 - LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f552b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16e6deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9bc342",
   "metadata": {},
   "source": [
    "Matrix 사용 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05370983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   11325.9121\n",
      "   50 |     135.3618\n",
      "  100 |      11.1817\n",
      "  150 |       9.7940\n",
      "  200 |       9.7687\n",
      "  250 |       9.7587\n",
      "  300 |       9.7489\n",
      "  350 |       9.7389\n",
      "  400 |       9.7292\n",
      "  450 |       9.7194\n",
      "  500 |       9.7096\n",
      "  550 |       9.6999\n",
      "  600 |       9.6903\n",
      "  650 |       9.6806\n",
      "  700 |       9.6709\n",
      "  750 |       9.6612\n",
      "  800 |       9.6517\n",
      "  850 |       9.6421\n",
      "  900 |       9.6325\n",
      "  950 |       9.6229\n",
      " 1000 |       9.6134\n"
     ]
    }
   ],
   "source": [
    " #data and label (5,3)⋅(?,?)=(5,1)\n",
    "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
    "Y  = [152., 185., 180., 196., 142.]\n",
    "\n",
    "#random weights -> (3,1) \n",
    "w1 = tf.Variable(tf.random.normal((1,)))\n",
    "w2 = tf.Variable(tf.random.normal((1,)))\n",
    "w3 = tf.Variable(tf.random.normal((1,)))\n",
    "b  = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "#임의의 작은 값 할당\n",
    "learning_rate = 0.000001\n",
    "\n",
    "#Gradient descent 구현 (weight 계속 업데이트) - 1001번 반복\n",
    "for i in range(1000+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape: #변수 변화 정보 tape기록\n",
    "        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y)) #오차 제곱의 평균값\n",
    "    #W1, W2에 대한 gradient 값 구해서 각각 할당\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    #구한 gradient값 빼서 업데이트 (gradient * learning_rate)\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy())) #cost만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4fcb5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 335.280823 |    -4.0663 |     1.1220 |  -6.065215\n",
      "   50 |  76.037262 |    -0.8001 |     1.6209 |  -4.978779\n",
      "  100 |  18.959263 |     0.7151 |     1.8781 |  -4.429109\n",
      "  150 |   6.310240 |     1.4125 |     2.0104 |  -4.134423\n",
      "  200 |   3.445082 |     1.7284 |     2.0768 |  -3.961648\n",
      "  250 |   2.743659 |     1.8667 |     2.1075 |  -3.847750\n",
      "  300 |   2.525401 |     1.9225 |     2.1184 |  -3.762738\n",
      "  350 |   2.417754 |     1.9402 |     2.1181 |  -3.692262\n",
      "  400 |   2.337300 |     1.9403 |     2.1114 |  -3.629400\n",
      "  450 |   2.264998 |     1.9325 |     2.1008 |  -3.570778\n",
      "  500 |   2.196328 |     1.9213 |     2.0881 |  -3.514729\n",
      "  550 |   2.130126 |     1.9085 |     2.0741 |  -3.460409\n",
      "  600 |   2.066037 |     1.8953 |     2.0595 |  -3.407385\n",
      "  650 |   2.003917 |     1.8819 |     2.0444 |  -3.355424\n",
      "  700 |   1.943679 |     1.8686 |     2.0293 |  -3.304398\n",
      "  750 |   1.885258 |     1.8555 |     2.0141 |  -3.254230\n",
      "  800 |   1.828595 |     1.8425 |     1.9990 |  -3.204873\n",
      "  850 |   1.773636 |     1.8297 |     1.9841 |  -3.156293\n",
      "  900 |   1.720329 |     1.8171 |     1.9693 |  -3.108468\n",
      "  950 |   1.668625 |     1.8048 |     1.9547 |  -3.061379\n",
      " 1000 |   1.618474 |     1.7926 |     1.9403 |  -3.015011\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0) \n",
    "\n",
    "#data and label (5,2)⋅(?,?)=(5,1)\n",
    "x1_data = [1, 0, 3, 0, 5]\n",
    "x2_data = [0, 2, 0, 4, 0]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "#random weights -> (2,1) \n",
    "W1 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "W2 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "b  = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "\n",
    "#임의의 작은 값 할당\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "#Gradient descent 구현 (weight 계속 업데이트) - 1001번 반복\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape: #변수 변화 정보 tape기록\n",
    "        hypothesis = W1 * x1_data + W2 * x2_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data)) #오차 제곱의 평균값\n",
    "        #W1, W2에 대한 gradient 값 구해서 각각 할당\n",
    "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])\n",
    "    #구한 gradient값 빼서 업데이트 (gradient * learning_rate)\n",
    "    W1.assign_sub(learning_rate * W1_grad)\n",
    "    W2.assign_sub(learning_rate * W2_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce0e156",
   "metadata": {},
   "source": [
    "GradientTape()\n",
    "- with 구문 안 변수들의 변화 정보를 tape에 기록함\n",
    "\n",
    "tape.gradient(A, [b.c])\n",
    "\n",
    "- 함수(A)에 대해 변수들([b,c])에 대한 개별 미분값을 구해 tuple로 반환\n",
    "\n",
    "A.assign_sub(B)\n",
    "\n",
    "- A = A - B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e994a",
   "metadata": {},
   "source": [
    "Matrix 사용 O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd95258",
   "metadata": {},
   "source": [
    "slice 메소드\n",
    "- data[Row ,Column ]\n",
    "  - \":\" -> 처음 ~ 끝까지\n",
    "  - \" :-1\" -> 처음 ~ 마지막 전 까지\n",
    "  - \" [-1]\" -> 마지막 \"만\"\n",
    "  \n",
    "tf.matmul(X, W)\n",
    "- 내적함수\n",
    "- X inner pot W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db5a7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62785ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 |  5455.5903\n",
      "  100 |    31.7443\n",
      "  200 |    30.9326\n",
      "  300 |    30.7894\n",
      "  400 |    30.6468\n",
      "  500 |    30.5055\n",
      "  600 |    30.3644\n",
      "  700 |    30.2242\n",
      "  800 |    30.0849\n",
      "  900 |    29.9463\n",
      " 1000 |    29.8081\n",
      " 1100 |    29.6710\n",
      " 1200 |    29.5348\n",
      " 1300 |    29.3989\n",
      " 1400 |    29.2641\n",
      " 1500 |    29.1299\n",
      " 1600 |    28.9961\n",
      " 1700 |    28.8634\n",
      " 1800 |    28.7313\n",
      " 1900 |    28.5997\n",
      " 2000 |    28.4689\n"
     ]
    }
   ],
   "source": [
    "# X=(5*3) , Y=(5,1)\n",
    "data = np.array([\n",
    "    # X1,   X2,    X3,   y\n",
    "    [ 73.,  80.,  75., 152. ],\n",
    "    [ 93.,  88.,  93., 185. ],\n",
    "    [ 89.,  91.,  90., 180. ],\n",
    "    [ 96.,  98., 100., 196. ],\n",
    "    [ 73.,  66.,  70., 142. ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# np의 slicing 사용 -> 데이터 분할\n",
    "X = data[:, :-1] # input\n",
    "y = data[:, [-1]] # 출력=label=정답\n",
    "\n",
    "#X Column = 3, Y Column = 1 --> W(3,1)\n",
    "W = tf.Variable(tf.random.normal((3, 1)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "# hypothesis function : 가설 함수\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b #이후 b는 생략 가능\n",
    "\n",
    "print(\"epoch | cost\")\n",
    "\n",
    "#gradient descent- 2001번 반복 -> W 업데이트\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
    "\n",
    "    # calculates the gradients of the loss\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "\n",
    "    # W update도 한 줄로 끝남!\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb96796",
   "metadata": {},
   "source": [
    "28에서 더이상 감소 X -> 최적화 완료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46e560",
   "metadata": {},
   "source": [
    "# LEC 5 - LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25cc5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dec5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용할 data (x,label(y) 선언)\n",
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_train = xy[:, 0:-1]\n",
    "y_train = xy[:, [-1]]\n",
    "\n",
    "#테스트 데이터\n",
    "x_test = np.array([[5, 2]], dtype=np.float32)\n",
    "y_test = np.array([[1]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38828420",
   "metadata": {},
   "source": [
    "- **batch size**\n",
    "    - 한 번의 batch마다 주는 데이터 샘플의 size \n",
    "    - **batch** :보통 mini-batch라고 표현. 나눠진 데이터 셋\n",
    "    - **iteration마다 주는 데이터 사이즈**\n",
    "    - iteration: epoch를 나누어서 실행하는 횟수\n",
    "    \n",
    "- **.from_tensor_slices**\n",
    "    - 리스트, 넘파이, 텐서플로우 자료형에서 -> 데이터셋 만들기 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f733cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset-> 우리가 원하는 x값, y값을 실제 x길이만큼 batch size로 해서 학습할 data 값 가져옴\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n",
    "#원하는 모델 선언\n",
    "W = tf.Variable(tf.random.normal((8, 1)), name='weight')\n",
    "b = tf.Variable(tf.random.normal((1,)), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08633802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.6217\n",
      "Iter: 100, Loss: 0.6143\n",
      "Iter: 200, Loss: 0.6074\n",
      "Iter: 300, Loss: 0.6009\n",
      "Iter: 400, Loss: 0.5949\n",
      "Iter: 500, Loss: 0.5892\n",
      "Iter: 600, Loss: 0.5839\n",
      "Iter: 700, Loss: 0.5789\n",
      "Iter: 800, Loss: 0.5742\n",
      "Iter: 900, Loss: 0.5698\n",
      "Iter: 1000, Loss: 0.5657\n"
     ]
    }
   ],
   "source": [
    "#logistic_regression에 대한 hypothesis\n",
    "def logistic_regression(features):               \n",
    "    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
    "    return hypothesis\n",
    "\n",
    "# cost = hypothesis - label(y) \n",
    "# cost(hθ, (x),y) = -ylog( hθ(x) ) - (1-y)log( 1- hθ(x) )\n",
    "def loss_fn(hypothesis, features, labels):\n",
    "    hypothesis = logistic_regression(features) #코드 오류 방지위해 한번 더\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "#learning rate로 oprtimizer 선언\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# 학습을 위한 함수\n",
    "def grad(hypothesis, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(logistic_regression(features),features,labels)\n",
    "    return tape.gradient(loss_value, [W,b])\n",
    "\n",
    "# 실제 학습 시작\n",
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels  in iter(dataset): #dataset을 토대로 iterator을 돌림 -> x, y값 대입\n",
    "        #가설(모델값), x, y로 gradient 구해서\n",
    "        grads = grad(logistic_regression(features), features, labels)\n",
    "        #gradient descent 알고리즘을 통해 gradient가 최소가 되는(=cost가 최소인) 가설값 만드는 W,b 구하기 \n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
    "        if step % 100 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd65094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
