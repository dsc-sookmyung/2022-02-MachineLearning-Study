{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1274adb9",
      "metadata": {
        "id": "1274adb9"
      },
      "source": [
        "# 04 Multi-variable linear regression\n",
        "## Hypothesis\n",
        "$$ H(x_1, x_2, x_3) = w_1x_1 + w_2x_2 + w_3x_3 + b $$\n",
        "\n",
        "## Cost function\n",
        "$$ cost(W, b) = \\frac{1}{m}\\sum_{i=1}^m (H(x_1, x_2, x_3)-y_i)^2$$\n",
        "\n",
        "## Multi-variable\n",
        "- 변수의 개수가 늘어나면 변수의 개수와 가중치의 개수가 늘어남\n",
        "\n",
        "## Matrix multiplication\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "## Hypothesis using matrix\n",
        "$$ (x_1 x_2 x_3) · (w_1 w_2 w_3)^T = (x_1w_1 + x_2w_2 + x_3w_3) $$\n",
        "$$ H(X) = XW $$\n",
        "- 앞 matrix의 열의 개수와 뒤 matrix의 행의 개수가 일치해야 함\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "## Many x instances\n",
        "- data의 instance가 많은 경우에도 동일하게 표현 가능\n",
        "- matrix를 쓰는 큰 장점\n",
        "\n",
        "## Hypothesis using matrix (n output)\n",
        "$$ [n, 3] · [?, ?] = [n, 2] $$\n",
        "- n은 instance의 개수, 2는 결과 값의 개수\n",
        "- 이 때 W[?, ?] => [3, 2]\n",
        "\n",
        "## WX vs XW\n",
        "- Lecture (theory)\n",
        "$$ H(x) = Wx + b $$\n",
        "- Implementation (TensorFlow)\n",
        "$$ H(X) = XW $$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae64f6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bae64f6b",
        "outputId": "a3868d27-a51f-4437-ca2d-077ff9484ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebdb04ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebdb04ec",
        "outputId": "601b5ade-377d-43a9-bb57-105257b185c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0 |   10187.6270\n",
            "   50 |     140.5898\n",
            "  100 |      29.0406\n",
            "  150 |      27.7368\n",
            "  200 |      27.6563\n",
            "  250 |      27.5897\n",
            "  300 |      27.5234\n",
            "  350 |      27.4572\n",
            "  400 |      27.3913\n",
            "  450 |      27.3254\n",
            "  500 |      27.2598\n",
            "  550 |      27.1944\n",
            "  600 |      27.1291\n",
            "  650 |      27.0640\n",
            "  700 |      26.9992\n",
            "  750 |      26.9344\n",
            "  800 |      26.8699\n",
            "  850 |      26.8055\n",
            "  900 |      26.7414\n",
            "  950 |      26.6774\n",
            " 1000 |      26.6136\n"
          ]
        }
      ],
      "source": [
        "# data and label\n",
        "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
        "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
        "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
        "Y  = [152., 185., 180., 196., 142.]\n",
        "\n",
        "# random weights\n",
        "w1 = tf.Variable(tf.random.normal([1]))\n",
        "w2 = tf.Variable(tf.random.normal([1]))\n",
        "w3 = tf.Variable(tf.random.normal([1]))\n",
        "b  = tf.Variable(tf.random.normal([1]))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "for i in range(1000+1):\n",
        "    # tf.GradientTape() to record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "    # calculates the gradients of the cost\n",
        "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
        "    \n",
        "    # update w1,w2,w3 and b\n",
        "    w1.assign_sub(learning_rate * w1_grad)\n",
        "    w2.assign_sub(learning_rate * w2_grad)\n",
        "    w3.assign_sub(learning_rate * w3_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "990ced32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "990ced32",
        "outputId": "44533166-c6a0-4722-cfe5-c50027b39f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch | cost\n",
            "    0 | 37361.2617\n",
            "  100 |     9.1569\n",
            "  200 |     4.5344\n",
            "  300 |     4.5104\n",
            "  400 |     4.4870\n",
            "  500 |     4.4638\n",
            "  600 |     4.4408\n",
            "  700 |     4.4178\n",
            "  800 |     4.3950\n",
            "  900 |     4.3723\n",
            " 1000 |     4.3497\n",
            " 1100 |     4.3272\n",
            " 1200 |     4.3049\n",
            " 1300 |     4.2827\n",
            " 1400 |     4.2606\n",
            " 1500 |     4.2386\n",
            " 1600 |     4.2168\n",
            " 1700 |     4.1950\n",
            " 1800 |     4.1734\n",
            " 1900 |     4.1519\n",
            " 2000 |     4.1305\n"
          ]
        }
      ],
      "source": [
        "data = np.array([\n",
        "    # X1,   X2,    X3,   y\n",
        "    [ 73.,  80.,  75., 152. ],\n",
        "    [ 93.,  88.,  93., 185. ],\n",
        "    [ 89.,  91.,  90., 180. ],\n",
        "    [ 96.,  98., 100., 196. ],\n",
        "    [ 73.,  66.,  70., 142. ]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# slice data\n",
        "X = data[:, :-1]\n",
        "y = data[:, [-1]]\n",
        "\n",
        "W = tf.Variable(tf.random.normal((3, 1)))\n",
        "b = tf.Variable(tf.random.normal((1,)))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "# hypothesis, prediction function\n",
        "def predict(X):\n",
        "    return tf.matmul(X, W) + b\n",
        "\n",
        "print(\"epoch | cost\")\n",
        "\n",
        "n_epochs = 2000\n",
        "for i in range(n_epochs+1):\n",
        "    # tf.GradientTape() to record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
        "\n",
        "    # calculates the gradients of the loss\n",
        "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "    # updates parameters (W and b)\n",
        "    W.assign_sub(learning_rate * W_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a7aeec",
      "metadata": {
        "id": "44a7aeec"
      },
      "source": [
        "# 05 Logistic Regression\n",
        "## What is Logistic Regression?\n",
        "### Logistic vs Linear\n",
        "- 구분된 데이터와 연속적인 데이터\n",
        "![image.png](attachment:image.png)\n",
        "```\n",
        "Logistic_Y = [[0], [0],[0],[1],[1],[1]]\n",
        "Linear_Y = [828.659973, 833.450012, 819.23999, 828.349976] #Numeric\n",
        "```\n",
        "\n",
        "## How to solve?\n",
        "### Hypothesis Representation\n",
        "- Linear가 아닌 형태로 표현해야 함\n",
        "```\n",
        "hypothesis=tf.matmul(X, Θ) + b #linear Θ is an [1xn+1] matrix / Θ are parameters\n",
        "```\n",
        "- ![image-2.png](attachment:image-2.png)\n",
        "\n",
        "### Sigmoid/Logistic Function\n",
        "$$ g(z) = 1/(1+e^{-z}) $$ \n",
        "```\n",
        "hypothesis = tf.sigmoid(z)\n",
        "hypothesis = tf.div(1., 1.+ tf.exp(z))\n",
        "```\n",
        "![image-3.png](attachment:image-3.png)\n",
        "\n",
        "### Decision Boundary\n",
        "- Decision Boundary를 결정\n",
        "```\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.int32)\n",
        "```\n",
        "\n",
        "### Cost Function\n",
        "- 학습을 통해서 최적의 모델을 찾아야 함\n",
        "- h(x) = y then Cost = 0\n",
        "$$ cost(h(x),y) = -ylog(h(x)) - (1-y)log(1-h(x)) $$\n",
        "```\n",
        "def loss_fn(hypothesis, labels):\n",
        "    cost = -tf.reduce_mean(labels * tf.log(hypothesis) + (1-lables) * tf.log(1-hypothesis))\n",
        "    return cost\n",
        "```\n",
        "\n",
        "### Optimizer (Gradient Descent)\n",
        "- How to minimize the cost function?\n",
        "```\n",
        "def grad(hypothesis, label):\n",
        "    with tf.GradientTape90 as tape:\n",
        "        loss_value = loss_fn(hypothesis, labels)\n",
        "    return tape.gradient(loss_value, [W, b])\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4bc200",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb4bc200",
        "outputId": "49a3da84-20ee-4f8d-c8cd-34a22755da0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(777)  # for reproducibility\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c968343b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "c968343b",
        "outputId": "0b982dfd-0b89-4e38-e1b8-6bd07c6a45a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "x_train = [[1., 2.],\n",
        "          [2., 3.],\n",
        "          [3., 1.],\n",
        "          [4., 3.],\n",
        "          [5., 3.],\n",
        "          [6., 2.]]\n",
        "y_train = [[0.],\n",
        "          [0.],\n",
        "          [0.],\n",
        "          [1.],\n",
        "          [1.],\n",
        "          [1.]]\n",
        "\n",
        "x_test = [[5.,2.]]\n",
        "y_test = [[1.]]\n",
        "\n",
        "\n",
        "x1 = [x[0] for x in x_train]\n",
        "x2 = [x[1] for x in x_train]\n",
        "\n",
        "colors = [int(y[0] % 3) for y in y_train]\n",
        "plt.scatter(x1,x2, c=colors , marker='^')\n",
        "plt.scatter(x_test[0][0],x_test[0][1], c=\"red\")\n",
        "\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c01a43b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c01a43b9",
        "outputId": "f51bfb85-cb70-4672-d610-8fb3592076b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0, Loss: 0.6874\n",
            "Iter: 100, Loss: 0.5776\n",
            "Iter: 200, Loss: 0.5349\n",
            "Iter: 300, Loss: 0.5054\n",
            "Iter: 400, Loss: 0.4838\n",
            "Iter: 500, Loss: 0.4671\n",
            "Iter: 600, Loss: 0.4535\n",
            "Iter: 700, Loss: 0.4420\n",
            "Iter: 800, Loss: 0.4319\n",
            "Iter: 900, Loss: 0.4228\n",
            "Iter: 1000, Loss: 0.4144\n",
            "Testset Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat()\n",
        "\n",
        "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
        "b = tf.Variable(tf.zeros([1]), name='bias')\n",
        "\n",
        "def logistic_regression(features):\n",
        "    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
        "    return hypothesis\n",
        "\n",
        "def loss_fn(hypothesis, features, labels):\n",
        "    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "    return cost\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "def accuracy_fn(hypothesis, labels):\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
        "    return accuracy\n",
        "\n",
        "def grad(features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(logistic_regression(features),features,labels)\n",
        "    return tape.gradient(loss_value, [W,b])\n",
        "\n",
        "EPOCHS = 1001\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in iter(dataset):\n",
        "        grads = grad(features, labels)\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "        if step % 100 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
        "test_acc = accuracy_fn(logistic_regression(x_test),y_test)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}