{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd21cf9",
   "metadata": {},
   "source": [
    "https://busy-tote-498.notion.site/rnn-a6dab994c73f40c08b1c65d6b6daa38a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1537ca",
   "metadata": {},
   "source": [
    "### many to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a627a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alaco\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential,Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "%matplotlib inline\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b536afc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', ' ', 'a', 'b', 'd', 'e', 'g', 'o', 'r', 's', 'w']\n",
      "{0: '', 1: ' ', 2: 'a', 3: 'b', 4: 'd', 5: 'e', 6: 'g', 7: 'o', 8: 'r', 9: 's', 10: 'w'}\n",
      "{'': 0, ' ': 1, 'a': 2, 'b': 3, 'd': 4, 'e': 5, 'g': 6, 'o': 7, 'r': 8, 's': 9, 'w': 10}\n"
     ]
    }
   ],
   "source": [
    "#예시\n",
    "words = ['good','bad','worse','so good']\n",
    "y_data=[1,0,0,1]\n",
    "\n",
    "#token dictionary 만들기\n",
    "char_set=['']+sorted(list(set(''.join(words))))\n",
    "idx2char={idx:char for idx,char in enumerate(char_set)}\n",
    "char2idx={char:idx for idx,char in enumerate(char_set)}\n",
    "\n",
    "print(char_set)\n",
    "print(idx2char)\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc477ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 7, 7, 4], [3, 2, 4], [10, 7, 8, 9, 5], [9, 7, 1, 6, 7, 7, 4]]\n",
      "[4, 3, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "#good -> 6 7 7 4\n",
    "\n",
    "#sequence of tokens -> sequence of indices\n",
    "x_data=list(map(lambda word: [char2idx.get(char) for char in word], words))\n",
    "x_data_len= list(map(lambda word: len(word), x_data))\n",
    "\n",
    "print(x_data)\n",
    "print(x_data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c08b78b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  7  7  4  0  0  0  0  0  0]\n",
      " [ 3  2  4  0  0  0  0  0  0  0]\n",
      " [10  7  8  9  5  0  0  0  0  0]\n",
      " [ 9  7  1  6  7  7  4  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "max_sequence= 10\n",
    "#padding='post' 히면 패딩할때 뒤로 0으로 채움, pad_sequence 가 기본적으로 앞으로 채우기때문\n",
    "x_data= pad_sequences(sequences=x_data, maxlen= max_sequence, padding='post',truncating='post')\n",
    "\n",
    "print(x_data)\n",
    "#print(x_data_len)\n",
    "#print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42b84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#many to one 만들기\n",
    "input_dim= len(char2idx)\n",
    "output_dim= len(char2idx)\n",
    "one_hot= np.eye(len(char2idx)) #정방행렬 만들기\n",
    "hidden_size=10\n",
    "num_classes=2 #긍정아니면 부정\n",
    "\n",
    "model=Sequential()\n",
    "model.add(layers.Embedding(input_dim=input_dim, output_dim=output_dim,\n",
    "                          trainable=False, mask_zero=True, input_length=max_sequence,\n",
    "                          embeddings_initializer=keras.initializers.Constant(one_hot)))\n",
    "model.add(layers.SimpleRNN(units=hidden_size))\n",
    "model.add(layers.Dense(units=num_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "803fbe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 11)            121       \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 10)                220       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 363\n",
      "Trainable params: 242\n",
      "Non-trainable params: 121\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec411073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating loss function\n",
    "def loss_fn(model, x, y):\n",
    "    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true=y, y_pred=model(x), from_logits=True))\n",
    "\n",
    "# creating an optimizer\n",
    "lr = .01\n",
    "epochs = 30\n",
    "batch_size = 2\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79b08ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 10), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "tr_dataset=tf.data.Dataset.from_tensor_slices((x_data,y_data))\n",
    "tr_dataset=tr_dataset.shuffle(buffer_size=4)\n",
    "tr_dataset=tr_dataset.batch(batch_size=batch_size)\n",
    "print(tr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6522ec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5, tr_loss:0.008\n",
      "epoch: 10, tr_loss:0.004\n",
      "epoch: 15, tr_loss:0.003\n",
      "epoch: 20, tr_loss:0.002\n",
      "epoch: 25, tr_loss:0.002\n",
      "epoch: 30, tr_loss:0.001\n"
     ]
    }
   ],
   "source": [
    "tr_loss_hist=[]\n",
    "for epoch in range(epochs):\n",
    "    avg_tr_loss=0\n",
    "    tr_step=0\n",
    "    \n",
    "    for x_mb,y_mb in tr_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tr_loss= loss_fn(model,x=x_mb,y=y_mb)\n",
    "        grads= tape.gradient(target=tr_loss, sources= model.variables)\n",
    "        opt.apply_gradients(grads_and_vars= zip(grads, model.variables))\n",
    "        avg_tr_loss +=tr_loss\n",
    "        tr_step +=1\n",
    "    else:\n",
    "        avg_tr_loss/=tr_step\n",
    "        tr_loss_hist.append(avg_tr_loss)\n",
    "        \n",
    "    if(epoch+1)%5==0:\n",
    "        print('epoch:{:3}, tr_loss:{:.3f}'.format(epoch+1,avg_tr_loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75d9e700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "acc:1.00\n"
     ]
    }
   ],
   "source": [
    "y_hat=model.predict(x_data)\n",
    "y_hat= np.argmax(y_hat,axis=-1)\n",
    "print('acc:{:.2f}'.format(np.mean(y_hat==y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b4d4d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25fd8683700>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tr_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea4903",
   "metadata": {},
   "source": [
    "## many to one stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c253e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example data\n",
    "sentences = ['What I cannot create, I do not understand.',\n",
    "             'Intellecuals solve problems, geniuses prevent them',\n",
    "             'A person who never made a mistake never tied anything new.',\n",
    "             'The same equations have the same solutions.']\n",
    "y_data = [1,0,0,1] # 1: richard feynman, 0: albert einstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0914bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, ' ': 1, ',': 2, '.': 3, 'A': 4, 'I': 5, 'T': 6, 'W': 7, 'a': 8, 'b': 9, 'c': 10, 'd': 11, 'e': 12, 'g': 13, 'h': 14, 'i': 15, 'k': 16, 'l': 17, 'm': 18, 'n': 19, 'o': 20, 'p': 21, 'q': 22, 'r': 23, 's': 24, 't': 25, 'u': 26, 'v': 27, 'w': 28, 'y': 29}\n"
     ]
    }
   ],
   "source": [
    "char_set = [''] + sorted(list(set(''.join(sentences))))\n",
    "idx2char = {idx : char for idx, char in enumerate(char_set)}\n",
    "char2idx = {char : idx for idx, char in enumerate(char_set)}\n",
    "\n",
    "#print(char_set)\n",
    "#print(idx2char)\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "254d77d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 14, 8, 25, 1, 5, 1, 10, 8, 19, 19, 20, 25, 1, 10, 23, 12, 8, 25, 12, 2, 1, 5, 1, 11, 20, 1, 19, 20, 25, 1, 26, 19, 11, 12, 23, 24, 25, 8, 19, 11, 3], [5, 19, 25, 12, 17, 17, 12, 10, 26, 8, 17, 24, 1, 24, 20, 17, 27, 12, 1, 21, 23, 20, 9, 17, 12, 18, 24, 2, 1, 13, 12, 19, 15, 26, 24, 12, 24, 1, 21, 23, 12, 27, 12, 19, 25, 1, 25, 14, 12, 18], [4, 1, 21, 12, 23, 24, 20, 19, 1, 28, 14, 20, 1, 19, 12, 27, 12, 23, 1, 18, 8, 11, 12, 1, 8, 1, 18, 15, 24, 25, 8, 16, 12, 1, 19, 12, 27, 12, 23, 1, 25, 15, 12, 11, 1, 8, 19, 29, 25, 14, 15, 19, 13, 1, 19, 12, 28, 3], [6, 14, 12, 1, 24, 8, 18, 12, 1, 12, 22, 26, 8, 25, 15, 20, 19, 24, 1, 14, 8, 27, 12, 1, 25, 14, 12, 1, 24, 8, 18, 12, 1, 24, 20, 17, 26, 25, 15, 20, 19, 24, 3]]\n"
     ]
    }
   ],
   "source": [
    "# converting sequence of tokens to sequence of indices\n",
    "x_data = list(map(lambda sentence : [char2idx.get(char) for char in sentence], sentences))\n",
    "x_data_len = list(map(lambda sentence : len(sentence), sentences))\n",
    "\n",
    "print(x_data)\n",
    "#print(x_data_len)\n",
    "#print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19b371d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 14  8 25  1  5  1 10  8 19 19 20 25  1 10 23 12  8 25 12  2  1  5  1\n",
      "  11 20  1 19 20 25  1 26 19 11 12 23 24 25  8 19 11  3  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]\n",
      " [ 5 19 25 12 17 17 12 10 26  8 17 24  1 24 20 17 27 12  1 21 23 20  9 17\n",
      "  12 18 24  2  1 13 12 19 15 26 24 12 24  1 21 23 12 27 12 19 25  1 25 14\n",
      "  12 18  0  0  0  0  0]\n",
      " [ 4  1 21 12 23 24 20 19  1 28 14 20  1 19 12 27 12 23  1 18  8 11 12  1\n",
      "   8  1 18 15 24 25  8 16 12  1 19 12 27 12 23  1 25 15 12 11  1  8 19 29\n",
      "  25 14 15 19 13  1 19]\n",
      " [ 6 14 12  1 24  8 18 12  1 12 22 26  8 25 15 20 19 24  1 14  8 27 12  1\n",
      "  25 14 12  1 24  8 18 12  1 24 20 17 26 25 15 20 19 24  3  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# padding the sequence of indices\n",
    "max_sequence = 55\n",
    "x_data = pad_sequences(sequences = x_data, maxlen = max_sequence,\n",
    "                       padding = 'post', truncating = 'post')\n",
    "#truncating= 값이 n개 이상일경우 뒤에 값 삭제\n",
    "\n",
    "# checking data\n",
    "print(x_data)\n",
    "#print(x_data_len)\n",
    "#print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e51ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating stacked rnn for \"many to one\" classification with dropout\n",
    "num_classes = 2\n",
    "hidden_dims = [10,10]\n",
    "\n",
    "input_dim = len(char2idx)\n",
    "output_dim = len(char2idx)\n",
    "one_hot = np.eye(len(char2idx))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=input_dim, output_dim=output_dim,\n",
    "                           trainable=False, mask_zero=True, input_length=max_sequence,\n",
    "                           embeddings_initializer=keras.initializers.Constant(one_hot)))\n",
    "model.add(layers.SimpleRNN(units=hidden_dims[0], return_sequences=True))\n",
    "model.add(layers.TimeDistributed(layers.Dropout(rate = .2)))\n",
    "model.add(layers.SimpleRNN(units=hidden_dims[1]))\n",
    "model.add(layers.Dropout(rate = .2))\n",
    "model.add(layers.Dense(units=num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b76b779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 55, 30)            900       \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 55, 10)            410       \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 55, 10)           0         \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 10)                210       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,542\n",
      "Trainable params: 642\n",
      "Non-trainable params: 900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4fb36e",
   "metadata": {},
   "source": [
    "## many to many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f93a6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example data\n",
    "sentences = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "pos = [['pronoun', 'verb', 'adjective'],\n",
    "     ['noun', 'verb', 'adverb', 'adjective'],\n",
    "     ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun'],\n",
    "     ['noun', 'verb', 'adverb', 'adjective', 'verb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18c4c0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '', 1: 'I', 2: 'a', 3: 'changing', 4: 'deep', 5: 'difficult', 6: 'fast', 7: 'feel', 8: 'for', 9: 'framework', 10: 'hungry', 11: 'is', 12: 'learning', 13: 'tensorflow', 14: 'very'}\n"
     ]
    }
   ],
   "source": [
    "# creating a token dictionary for word\n",
    "word_list = sum(sentences, [])\n",
    "word_list = sorted(set(word_list))\n",
    "word_list = [''] + word_list\n",
    "word2idx = {word : idx for idx, word in enumerate(word_list)}\n",
    "idx2word = {idx : word for idx, word in enumerate(word_list)}\n",
    "\n",
    "#print(word2idx)\n",
    "print(idx2word)\n",
    "#print(len(idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4aaecd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '', 1: 'adjective', 2: 'adverb', 3: 'determiner', 4: 'noun', 5: 'preposition', 6: 'pronoun', 7: 'verb'}\n"
     ]
    }
   ],
   "source": [
    "# creating a token dictionary for part of speech\n",
    "pos_list = sum(pos, [])\n",
    "pos_list = sorted(set(pos_list))\n",
    "pos_list = [''] + pos_list\n",
    "pos2idx = {pos : idx for idx, pos in enumerate(pos_list)}\n",
    "idx2pos = {idx : pos for idx, pos in enumerate(pos_list)}\n",
    "\n",
    "#print(pos2idx)\n",
    "print(idx2pos)\n",
    "#print(len(pos2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4370dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]] [3, 4, 7, 5]\n"
     ]
    }
   ],
   "source": [
    "# converting sequence of tokens to sequence of indices\n",
    "max_sequence = 10\n",
    "x_data = list(map(lambda sentence : [word2idx.get(token) for token in sentence], sentences))\n",
    "y_data = list(map(lambda sentence : [pos2idx.get(token) for token in sentence], pos))\n",
    "\n",
    "# padding the sequence of indices\n",
    "x_data = pad_sequences(sequences = x_data, maxlen = max_sequence, padding='post')\n",
    "x_data_mask = ((x_data != 0) * 1).astype(np.float32)\n",
    "x_data_len = list(map(lambda sentence : len(sentence), sentences))\n",
    "\n",
    "y_data = pad_sequences(sequences = y_data, maxlen = max_sequence, padding='post')\n",
    "\n",
    "# checking data\n",
    "print(x_data, x_data_len)\n",
    "#print(x_data_mask)\n",
    "#print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b6cf3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating rnn for \"many to many\" sequence tagging\n",
    "num_classes = len(pos2idx)\n",
    "hidden_dim = 10\n",
    "\n",
    "input_dim = len(word2idx)\n",
    "output_dim = len(word2idx)\n",
    "one_hot = np.eye(len(word2idx))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=input_dim, output_dim=output_dim, mask_zero=True,\n",
    "                           trainable=False, input_length=max_sequence,\n",
    "                           embeddings_initializer=keras.initializers.Constant(one_hot)))\n",
    "model.add(layers.SimpleRNN(units=hidden_dim, return_sequences=True))\n",
    "model.add(layers.TimeDistributed(layers.Dense(units=num_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40b29972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 10, 15)            225       \n",
      "                                                                 \n",
      " simple_rnn_3 (SimpleRNN)    (None, 10, 10)            260       \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 10, 8)            88        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 573\n",
      "Trainable params: 348\n",
      "Non-trainable params: 225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1fa15731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating loss function\n",
    "def loss_fn(model, x, y, x_len, max_sequence):\n",
    "    masking = tf.sequence_mask(x_len, maxlen=max_sequence, dtype=tf.float32)\n",
    "    valid_time_step = tf.cast(x_len,dtype=tf.float32)    \n",
    "    sequence_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true=y, y_pred=model(x), from_logits=True) * masking    \n",
    "    sequence_loss = tf.reduce_sum(sequence_loss, axis=-1) / valid_time_step    \n",
    "    sequence_loss = tf.reduce_mean(sequence_loss)    \n",
    "    return sequence_loss\n",
    "\n",
    "# creating and optimizer\n",
    "lr = 0.1\n",
    "epochs = 30\n",
    "batch_size = 2 \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3669d704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 10), dtype=tf.int32, name=None), TensorSpec(shape=(None, 10), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# generating data pipeline\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data, x_data_len))\n",
    "tr_dataset = tr_dataset.shuffle(buffer_size=4)\n",
    "tr_dataset = tr_dataset.batch(batch_size = 2)\n",
    "\n",
    "print(tr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "016b65d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :   5, tr_loss : 0.108\n",
      "epoch :  10, tr_loss : 0.006\n",
      "epoch :  15, tr_loss : 0.002\n",
      "epoch :  20, tr_loss : 0.001\n",
      "epoch :  25, tr_loss : 0.000\n",
      "epoch :  30, tr_loss : 0.000\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "tr_loss_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_tr_loss = 0\n",
    "    tr_step = 0\n",
    "    \n",
    "    for x_mb, y_mb, x_mb_len in tr_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tr_loss = loss_fn(model, x=x_mb, y=y_mb, x_len=x_mb_len, max_sequence=max_sequence)\n",
    "        grads = tape.gradient(target=tr_loss, sources=model.variables)\n",
    "        opt.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "        avg_tr_loss += tr_loss\n",
    "        tr_step += 1\n",
    "    else:\n",
    "        avg_tr_loss /= tr_step\n",
    "        tr_loss_hist.append(avg_tr_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print('epoch : {:3}, tr_loss : {:.3f}'.format(epoch + 1, avg_tr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6aa3bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from pprint import pprint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7bf340cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "[['pronoun', 'verb', 'adjective', '', '', '', '', '', '', ''],\n",
      " ['noun', 'verb', 'adverb', 'adjective', '', '', '', '', '', ''],\n",
      " ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun', '', '', ''],\n",
      " ['noun', 'verb', 'adverb', 'adjective', 'verb', '', '', '', '', '']]\n",
      "[['pronoun', 'verb', 'adjective'],\n",
      " ['noun', 'verb', 'adverb', 'adjective'],\n",
      " ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun'],\n",
      " ['noun', 'verb', 'adverb', 'adjective', 'verb']]\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(x_data)\n",
    "yhat = np.argmax(yhat, axis=-1) * x_data_mask\n",
    "\n",
    "pprint(list(map(lambda row : [idx2pos.get(elm) for elm in row],yhat.astype(np.int32).tolist())), width = 120)\n",
    "pprint(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739d97f",
   "metadata": {},
   "source": [
    "## many to many bidirectional\n",
    "- model 안에 bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "029573e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating rnn for \"many to many\" sequence tagging\n",
    "num_classes = len(pos2idx)\n",
    "hidden_dim = 10\n",
    "\n",
    "input_dim = len(word2idx)\n",
    "output_dim = len(word2idx)\n",
    "one_hot = np.eye(len(word2idx))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(max_sequence,)))\n",
    "model.add(layers.Embedding(input_dim=input_dim, output_dim=output_dim, mask_zero=True,\n",
    "                           trainable=False, input_length=max_sequence,\n",
    "                           embeddings_initializer=keras.initializers.Constant(one_hot)))\n",
    "model.add(layers.Bidirectional(keras.layers.SimpleRNN(units=hidden_dim, return_sequences=True)))\n",
    "model.add(layers.TimeDistributed(layers.Dense(units=num_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49913c21",
   "metadata": {},
   "source": [
    "## seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9204ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "rc('font', family='AppleGothic') #for mac\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2011951",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b1e661f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '',\n",
      " 1: 'I',\n",
      " 2: 'a',\n",
      " 3: 'changing',\n",
      " 4: 'deep',\n",
      " 5: 'difficult',\n",
      " 6: 'fast',\n",
      " 7: 'feel',\n",
      " 8: 'for',\n",
      " 9: 'framework',\n",
      " 10: 'hungry',\n",
      " 11: 'is',\n",
      " 12: 'learning',\n",
      " 13: 'tensorflow',\n",
      " 14: 'very'}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for sources\n",
    "s_vocab = list(set(sum(sources, [])))\n",
    "s_vocab.sort()\n",
    "s_vocab = [''] + s_vocab\n",
    "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
    "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
    "\n",
    "#pprint(source2idx)\n",
    "pprint(idx2source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dcd137f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 2,\n",
      " '고프다': 3,\n",
      " '나는': 4,\n",
      " '딥러닝을': 5,\n",
      " '매우': 6,\n",
      " '배가': 7,\n",
      " '변화한다': 8,\n",
      " '빠르게': 9,\n",
      " '어렵다': 10,\n",
      " '위한': 11,\n",
      " '텐서플로우는': 12,\n",
      " '프레임워크이다': 13}\n",
      "{0: '',\n",
      " 1: '',\n",
      " 2: '',\n",
      " 3: '고프다',\n",
      " 4: '나는',\n",
      " 5: '딥러닝을',\n",
      " 6: '매우',\n",
      " 7: '배가',\n",
      " 8: '변화한다',\n",
      " 9: '빠르게',\n",
      " 10: '어렵다',\n",
      " 11: '위한',\n",
      " 12: '텐서플로우는',\n",
      " 13: '프레임워크이다'}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for targets\n",
    "t_vocab = list(set(sum(targets, [])))\n",
    "t_vocab.sort()\n",
    "t_vocab = ['', '', ''] + t_vocab\n",
    "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
    "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
    "\n",
    "pprint(target2idx)\n",
    "pprint(idx2target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0537d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
    "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
    "    \n",
    "    if mode == 'source':\n",
    "        # preprocessing for source (encoder)\n",
    "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
    "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
    "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        return s_len, s_input\n",
    "    \n",
    "    elif mode == 'target':\n",
    "        # preprocessing for target (decoder)\n",
    "        # input\n",
    "        t_input = list(map(lambda sentence : [''] + sentence + [''], sequences))\n",
    "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
    "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
    "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        # output\n",
    "        t_output = list(map(lambda sentence : sentence + [''], sequences))\n",
    "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
    "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        return t_len, t_input, t_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f922779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing for source\n",
    "s_max_len = 10\n",
    "s_len, s_input = preprocess(sequences = sources,\n",
    "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n",
    "print(s_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "05f6adcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  7  3  2  0  0  0  0  0  0  0]\n",
      " [ 2 12  6 10  2  0  0  0  0  0  0  0]\n",
      " [ 2 12  5 11 13  2  0  0  0  0  0  0]\n",
      " [ 2 12  6  9  8  2  0  0  0  0  0  0]] [[ 4  7  3  2  0  0  0  0  0  0  0  0]\n",
      " [12  6 10  2  0  0  0  0  0  0  0  0]\n",
      " [12  5 11 13  2  0  0  0  0  0  0  0]\n",
      " [12  6  9  8  2  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing for target\n",
    "t_max_len = 12\n",
    "t_len, t_input, t_output = preprocess(sequences = targets,\n",
    "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
    "print(t_input, t_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3cf0ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 200\n",
    "batch_size = 4\n",
    "learning_rate = .005\n",
    "total_step = epochs / batch_size\n",
    "buffer_size = 100\n",
    "n_batch = buffer_size//batch_size\n",
    "embedding_dim = 32\n",
    "units = 32\n",
    "\n",
    "# input\n",
    "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
    "data = data.shuffle(buffer_size = buffer_size)\n",
    "data = data.batch(batch_size = batch_size)\n",
    "# s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4486a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7ef0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f041357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "                \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ccd1f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
    "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    \n",
    "#     print(\"real: {}\".format(real))\n",
    "#     print(\"pred: {}\".format(pred))\n",
    "#     print(\"mask: {}\".format(mask))\n",
    "#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# creating optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# creating check point (Object-based saving)\n",
    "checkpoint_dir = './data_out/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder)\n",
    "\n",
    "# create writer for tensorboard\n",
    "#summary_writer = tf.summary.create_file_writer(logdir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "454cac4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Received a label value of 12 which is outside the valid range of [0, 12).  Label values: 4 12 12 12 [Op:SparseSoftmaxCrossEntropyWithLogits]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20504/1491700962.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mdec_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#using teacher forcing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20504/3016159570.py\u001b[0m in \u001b[0;36mloss_function\u001b[1;34m(real, pred)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mloss_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#     print(\"real: {}\".format(real))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7162\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7163\u001b[0m   \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7164\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Received a label value of 12 which is outside the valid range of [0, 12).  Label values: 4 12 12 12 [Op:SparseSoftmaxCrossEntropyWithLogits]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(s_input, hidden)\n",
    "\n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([target2idx['']] * batch_size, 1)\n",
    "            \n",
    "            #Teacher Forcing: feeding the target as the next input\n",
    "            for t in range(1, t_input.shape[1]):\n",
    "                \n",
    "                predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(t_input[:, t], predictions)\n",
    "            \n",
    "                dec_input = tf.expand_dims(t_input[:, t], 1) #using teacher forcing\n",
    "                \n",
    "        batch_loss = (loss / int(t_input.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradient = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradient, variables))\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        #save model every 10 epoch\n",
    "        print('Epoch {} Loss {:.4f} Batch Loss {:.4f}'.format(epoch,\n",
    "                                            total_loss / n_batch,\n",
    "                                            batch_loss.numpy()))\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1953727c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
