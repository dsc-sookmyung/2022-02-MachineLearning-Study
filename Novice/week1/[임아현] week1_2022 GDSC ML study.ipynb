{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7995ae72",
   "metadata": {},
   "source": [
    "### ML lec 01 - 기본적인 Machine Learning 의 용어와 개념 설명\n",
    "\n",
    "- Supervised learning: 레이블 되어있는 데이터로 학습 - training set\n",
    "- unsupervised learning : 레이블 되어있지 않은 데이터로 스스로 학습\n",
    " \n",
    " \n",
    "1) 시험 성적을 예측하는 시스템 성적 범위 0점~100점\n",
    "    - regression\n",
    "\n",
    "2) 둘 중에 하나를 고르는 Pass/non-pass based on time spent \n",
    "    - binary classification \n",
    "3) grade (A, B, C, E and F) \n",
    "    - multi. label classification\n",
    "     \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c3361",
   "metadata": {},
   "source": [
    "### [TensorFlow] Lec-02-Simple Liner Regression\n",
    "- Simple Liner Regression\n",
    "1) Regression : 데이터들은 전체의 평균으로 되돌아가려는/회귀하려는 특징이 있다.\n",
    "2) Linear regression : 선형회귀, 데이터를 가장 잘 대변하는 직선의 방정식을 찾는 것, 데이터 분포되어 있을 때 이를 가장 잘 대변하는 직선방정식의 기울기, y절편을 구하는 것\n",
    "3) Hypothesis 가설, 우리의 예측 : H(x) = Wx + b\n",
    "4) Cost, cost function : 실제 데이터와의 차이를 나타낸 부분 , H(x)-y 를 최소화하는 방법을 찾아야한다. \n",
    "5) Cost(W) = (에러값 = 가설(예측) - 실제값)을 제곱\n",
    "6) Error : 예측과 실제 값의 차이, cost 값은 error 제곱의 평균값\n",
    "7) Goal: minimize cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b683b",
   "metadata": {},
   "source": [
    "### TensorFlow Lab-02 Simple Linear Regression LAB\n",
    "\n",
    "- Cost = 에러 제곱의 평균 값으로 정의\n",
    "- Gradient descent (경사하강알고리즘) : 경사를 내려가면서 cost를 minimize하는 w와 b의 값을 찾는 것 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d2d4e",
   "metadata": {},
   "source": [
    "#error 제곱의 평균\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "#데이터준비\n",
    "x_data = [1,2,3,4,5]\n",
    "y_data = [1,2,3,4,5]\n",
    "\n",
    "\n",
    "#변수 w, b 초기화\n",
    "w= tf.Variable(2.9)\n",
    "b=tf.Variable(0.5)\n",
    "\n",
    "#learning_rate 상수로 지정\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(100+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis - W * x_data +b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "        \n",
    "    w_grad, b_grad = tape.gradient(cost, [w,b])\n",
    "    \n",
    "    #w,b 값을 지속적으로 갱신 업데이트\n",
    "    #여러번학습->오차없이 실제값과 거의 일치하도록\n",
    "    w.assign_sub(learning_rate * w_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    \n",
    "    #변하는 값 출력\n",
    "    if i % 10 == 0:\n",
    "        print(\"{:5}|{:10.4f}|{:10.4}|{:10.6f}\".format(i, w.numpy(), b.numpy(),cost))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e82f3b5",
   "metadata": {},
   "source": [
    "### [TensorFlow] Lec-03 Liner Regression and How to minimize cost\n",
    "- 비용이 작을수록 가설이 실제 데이터를 잘 반영하고 있음을 알 수 있다.\n",
    "- H(X) = Wx \n",
    "- Cost(W)= 오차 제곱의 평균\n",
    "\n",
    "\n",
    "##### What cost(W) looks like? \n",
    "\n",
    "w=0, cost(w)= 4.67\n",
    "w=1, cost(w)= 0\n",
    "w=2, cost(w)= 4.67\n",
    "w=3, cost(w)= 18.67\n",
    "\n",
    "머신러닝 -> cost 가 최저점을 찾아가는 과정\n",
    "\n",
    "\n",
    "##### Gradient descent algorithm\n",
    "- 경사를 따라내려가면서 최저점을 찾는 것\n",
    "- 변수가 여러개일때도 사용이 가능\n",
    "\n",
    "###### how it works?\n",
    "- 최초 임의로 w, b 값을 정한다.\n",
    "- 비용이 최소화되는 방향으로 w,b 를 지속적으로 바꾼다. \n",
    "- 최소점에 도달할 때까지 반복한다.\n",
    "- 기울기 = 0 비용이 최소화되는 지점\n",
    "- W값을 지속적으로 업데이트\n",
    "- W =  W - (비용을 미분한 값) X (특정상수:얼마만큼 빼줄지, 학습을 얼마나할지, learning rate)\n",
    "- learning rate가 작을수록 W 값이 적게 움직인다. \n",
    "\n",
    "기울기 -> 미분을 통해서 구할 수 있음\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70407ddb",
   "metadata": {},
   "source": [
    "### [TensorFlow] Lab-03 Liner Regression and How to minimize cost LAB\n",
    "이전 강의 복습)\n",
    "- Cost = ( 우리의 예측 값 - 실제 값)제곱의 평균한 값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ad5f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89b96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c1b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e58af43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
